{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sevginurbilgin/EDA/blob/main/FeatureExtraction_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp3qxaduw5S_"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:pass"
      ],
      "metadata": {
        "id": "D-9TJu0vLc7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "1cfb8b95-2586-4603-d46c-a91dd17ee2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-534b7a74019f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QSnbr_CWwsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ab3eb9-9beb-4dba-8b74-c37d987baf24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting heartpy\n",
            "  Downloading heartpy-1.2.7-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from heartpy) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from heartpy) (1.11.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from heartpy) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->heartpy) (1.16.0)\n",
            "Installing collected packages: heartpy\n",
            "Successfully installed heartpy-1.2.7\n",
            "Collecting neurokit2\n",
            "  Downloading neurokit2-0.2.7-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from neurokit2) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neurokit2) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from neurokit2) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neurokit2) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from neurokit2) (3.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->neurokit2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->neurokit2) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurokit2) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurokit2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurokit2) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurokit2) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurokit2) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurokit2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurokit2) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neurokit2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neurokit2) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->neurokit2) (1.16.0)\n",
            "Installing collected packages: neurokit2\n",
            "Successfully installed neurokit2-0.2.7\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.23.5)\n",
            "Collecting shap\n",
            "  Downloading shap-0.44.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (533 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.5/533.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.44.0 slicer-0.0.7\n",
            "Collecting mrmr_selection\n",
            "  Downloading mrmr_selection-0.2.8-py3-none-any.whl (15 kB)\n",
            "Collecting category-encoders (from mrmr_selection)\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (3.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (4.66.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.3.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.11.4)\n",
            "Requirement already satisfied: polars>=0.12.5 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (0.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->mrmr_selection) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->mrmr_selection) (2023.3.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars>=0.12.5->mrmr_selection) (4.5.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders->mrmr_selection) (0.14.1)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category-encoders->mrmr_selection) (0.5.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mrmr_selection) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mrmr_selection) (2.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category-encoders->mrmr_selection) (1.16.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category-encoders->mrmr_selection) (23.2)\n",
            "Installing collected packages: category-encoders, mrmr_selection\n",
            "Successfully installed category-encoders-2.6.3 mrmr_selection-0.2.8\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install heartpy\n",
        "!pip install neurokit2\n",
        "!pip install h5py\n",
        "!pip install shap\n",
        "!pip install mrmr_selection\n",
        "!pip install tensorflow\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.stats\n",
        "import heartpy as hp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from natsort import natsorted\n",
        "import neurokit2 as nk\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, accuracy_score, f1_score, auc\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from itertools import cycle\n",
        "import xgboost\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6ExIFz_Wr6E",
        "outputId": "5552cca2-5898-4bd7-897e-577500fb47ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYX5e1DQCGz8"
      },
      "outputs": [],
      "source": [
        "main_path='/content/drive/MyDrive/8Haziran2023_veriler/StroopTest/'\n",
        "\n",
        "output_path_graphs = '/content/drive/MyDrive/Tezim/CODE/8_Haziran/Analiz/GRAPHICS/'\n",
        "output_path_features = '/content/drive/MyDrive/Tezim/CODE/8_Haziran/Analiz/FEATURES/'\n",
        "\n",
        "output = '/content/drive/MyDrive/Tezim/CODE/8_Haziran/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(output+'Pickled_Data/'+'BVP_baseline.pkl', 'rb') as file:\n",
        "    BVP_baseline = pickle.load(file)\n",
        "with open(output+'Pickled_Data/'+'EDA_baseline.pkl', 'rb') as file:\n",
        "    EDA_baseline = pickle.load(file)\n",
        "with open(output+'Pickled_Data/'+'ST_baseline.pkl', 'rb') as file:\n",
        "    ST_baseline = pickle.load(file)\n",
        "\n",
        "with open(output+'Pickled_Data/'+'BVP_stroop.pkl', 'rb') as file:\n",
        "    BVP_stroop = pickle.load(file)\n",
        "with open(output+'Pickled_Data/'+'EDA_stroop.pkl', 'rb') as file:\n",
        "    EDA_stroop = pickle.load(file)\n",
        "with open(output+'Pickled_Data/'+'ST_stroop.pkl', 'rb') as file:\n",
        "    ST_stroop = pickle.load(file)\n",
        "\n",
        "with open(output+'Pickled_Data/'+'BVP_ns.pkl', 'rb') as file:\n",
        "    BVP_ns = pickle.load(file)\n",
        "with open(output+'Pickled_Data/'+'EDA_ns.pkl', 'rb') as file:\n",
        "    EDA_ns = pickle.load(file)\n",
        "with open(output+'Pickled_Data/'+'ST_ns.pkl', 'rb') as file:\n",
        "    ST_ns = pickle.load(file)"
      ],
      "metadata": {
        "id": "sfZFYVMHOpQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toplu_dfs_wo_id = [EDA_baseline,EDA_stroop,EDA_ns]\n",
        "toplu_dfs_wo_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7pO3aMu62KZ",
        "outputId": "aa4a963d-26c5-4d60-974a-d3676b28c300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[            0\n",
              "  72   0.447131\n",
              "  73   0.445850\n",
              "  74   0.447131\n",
              "  75   0.457380\n",
              "  76   0.457380\n",
              "  ..        ...\n",
              "  307  0.461224\n",
              "  308  0.457380\n",
              "  309  0.459942\n",
              "  310  0.457380\n",
              "  311  0.458661\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "              0\n",
              "  62   0.163981\n",
              "  63   0.161419\n",
              "  64   0.162700\n",
              "  65   0.162700\n",
              "  66   0.160137\n",
              "  ..        ...\n",
              "  297  0.166543\n",
              "  298  0.163981\n",
              "  299  0.167824\n",
              "  300  0.165262\n",
              "  301  0.166543\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "              0\n",
              "  63   3.018536\n",
              "  64   3.003162\n",
              "  65   2.968570\n",
              "  66   2.953196\n",
              "  67   2.935260\n",
              "  ..        ...\n",
              "  298  3.816101\n",
              "  299  3.802008\n",
              "  300  3.791759\n",
              "  301  3.773822\n",
              "  302  3.769979\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "              0\n",
              "  59   0.285685\n",
              "  60   0.289529\n",
              "  61   0.286966\n",
              "  62   0.288247\n",
              "  63   0.290810\n",
              "  ..        ...\n",
              "  294  0.298496\n",
              "  295  0.295934\n",
              "  296  0.297215\n",
              "  297  0.295934\n",
              "  298  0.298496\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "               0\n",
              "  57   14.512724\n",
              "  58   14.430734\n",
              "  59   14.334651\n",
              "  60   14.312873\n",
              "  61   14.293656\n",
              "  ..         ...\n",
              "  292  17.657148\n",
              "  293  17.558504\n",
              "  294  17.461140\n",
              "  295  17.358652\n",
              "  296  17.261290\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "               0\n",
              "  46   10.447077\n",
              "  47   10.485729\n",
              "  48   10.499821\n",
              "  49   10.279471\n",
              "  50   10.415268\n",
              "  ..         ...\n",
              "  281  11.053907\n",
              "  282  11.050064\n",
              "  283  11.048783\n",
              "  284  11.023161\n",
              "  285  11.010350\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "               0\n",
              "  92    9.303744\n",
              "  93    9.301183\n",
              "  94    9.315274\n",
              "  95    9.307588\n",
              "  96    9.312713\n",
              "  ..         ...\n",
              "  327  11.513139\n",
              "  328  11.492641\n",
              "  329  11.446522\n",
              "  330  11.443959\n",
              "  331  11.459332\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "               0\n",
              "  72   19.097666\n",
              "  73   19.138660\n",
              "  74   19.237305\n",
              "  75   19.318014\n",
              "  76   19.357729\n",
              "  ..         ...\n",
              "  307  24.762575\n",
              "  308  24.758732\n",
              "  309  24.788198\n",
              "  310  24.831755\n",
              "  311  24.895809\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "               0\n",
              "  65   10.832907\n",
              "  66   10.802160\n",
              "  67   10.525443\n",
              "  68   10.098837\n",
              "  69    9.948948\n",
              "  ..         ...\n",
              "  300  16.013391\n",
              "  301  16.127859\n",
              "  302  16.162449\n",
              "  303  16.281591\n",
              "  304  16.340521\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "               0\n",
              "  62    6.913774\n",
              "  63    7.264753\n",
              "  64    7.570899\n",
              "  65    7.928569\n",
              "  66    8.043651\n",
              "  ..         ...\n",
              "  297   9.783570\n",
              "  298  10.102526\n",
              "  299  10.514990\n",
              "  300  10.797017\n",
              "  301  10.875154\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "              0\n",
              "  149  0.408671\n",
              "  150  0.408671\n",
              "  151  0.404828\n",
              "  152  0.409952\n",
              "  153  0.406109\n",
              "  ..        ...\n",
              "  384  0.392017\n",
              "  385  0.393298\n",
              "  386  0.393298\n",
              "  387  0.397141\n",
              "  388  0.407390\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "              0\n",
              "  51   3.026791\n",
              "  52   3.021666\n",
              "  53   3.016542\n",
              "  54   3.015261\n",
              "  55   3.005012\n",
              "  ..        ...\n",
              "  286  1.828763\n",
              "  287  1.826201\n",
              "  288  1.818515\n",
              "  289  1.818515\n",
              "  290  1.810828\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "              0\n",
              "  51   0.362551\n",
              "  52   0.365113\n",
              "  53   0.366395\n",
              "  54   0.363832\n",
              "  55   0.367676\n",
              "  ..        ...\n",
              "  286  0.326680\n",
              "  287  0.329243\n",
              "  288  0.329243\n",
              "  289  0.326680\n",
              "  290  0.327962\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "              0\n",
              "  53   4.700006\n",
              "  54   4.741001\n",
              "  55   4.878079\n",
              "  56   5.100990\n",
              "  57   5.294436\n",
              "  ..        ...\n",
              "  288  6.735387\n",
              "  289  6.703360\n",
              "  290  6.690548\n",
              "  291  6.655959\n",
              "  292  6.690548\n",
              "  \n",
              "  [240 rows x 1 columns],\n",
              "              0\n",
              "  48   0.368957\n",
              "  49   0.365113\n",
              "  50   0.366395\n",
              "  51   0.367676\n",
              "  52   0.365113\n",
              "  ..        ...\n",
              "  283  0.357427\n",
              "  284  0.354865\n",
              "  285  0.359989\n",
              "  286  0.358708\n",
              "  287  0.356146\n",
              "  \n",
              "  [240 rows x 1 columns]],\n",
              " [            0\n",
              "  312  0.461224\n",
              "  313  0.458661\n",
              "  314  0.458661\n",
              "  315  0.457380\n",
              "  316  0.458661\n",
              "  ..        ...\n",
              "  787  0.431757\n",
              "  788  0.434319\n",
              "  789  0.434319\n",
              "  790  0.434319\n",
              "  791  0.435600\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "              0\n",
              "  302  0.166543\n",
              "  303  0.163981\n",
              "  304  0.166543\n",
              "  305  0.169105\n",
              "  306  0.165262\n",
              "  ..        ...\n",
              "  777  0.175511\n",
              "  778  0.174230\n",
              "  779  0.175511\n",
              "  780  0.175511\n",
              "  781  0.172948\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "              0\n",
              "  303  3.763573\n",
              "  304  3.757167\n",
              "  305  3.764854\n",
              "  306  3.849412\n",
              "  307  3.974967\n",
              "  ..        ...\n",
              "  778  5.325078\n",
              "  779  5.308423\n",
              "  780  5.302017\n",
              "  781  5.310986\n",
              "  782  5.350702\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "              0\n",
              "  299  0.299777\n",
              "  300  0.295934\n",
              "  301  0.298496\n",
              "  302  0.295934\n",
              "  303  0.295934\n",
              "  ..        ...\n",
              "  774  0.375362\n",
              "  775  0.376643\n",
              "  776  0.375362\n",
              "  777  0.376643\n",
              "  778  0.379206\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  297  17.183142\n",
              "  298  17.133179\n",
              "  299  17.112680\n",
              "  300  17.098589\n",
              "  301  17.126774\n",
              "  ..         ...\n",
              "  772  23.601938\n",
              "  773  23.704426\n",
              "  774  23.827412\n",
              "  775  23.858158\n",
              "  776  23.856876\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  286  11.009068\n",
              "  287  10.977041\n",
              "  288  10.974479\n",
              "  289  10.959105\n",
              "  290  10.936047\n",
              "  ..         ...\n",
              "  761   8.742623\n",
              "  762   8.754153\n",
              "  763   8.746467\n",
              "  764   8.754153\n",
              "  765   8.747747\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  332  11.467019\n",
              "  333  11.481112\n",
              "  334  11.490079\n",
              "  335  11.495204\n",
              "  336  11.490079\n",
              "  ..         ...\n",
              "  807  11.027602\n",
              "  808  11.063473\n",
              "  809  11.040413\n",
              "  810  11.017353\n",
              "  811  11.010948\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  312  24.927837\n",
              "  313  24.938086\n",
              "  314  24.980362\n",
              "  315  25.002142\n",
              "  316  24.957302\n",
              "  ..         ...\n",
              "  787  31.334467\n",
              "  788  31.279379\n",
              "  789  31.246071\n",
              "  790  31.147425\n",
              "  791  31.084652\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  305  16.352051\n",
              "  306  16.300808\n",
              "  307  16.223942\n",
              "  308  16.127859\n",
              "  309  16.088144\n",
              "  ..         ...\n",
              "  780  21.539495\n",
              "  781  21.490814\n",
              "  782  21.452620\n",
              "  783  21.412905\n",
              "  784  21.378315\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  302  10.814950\n",
              "  303  10.706070\n",
              "  304  10.549794\n",
              "  305  10.396081\n",
              "  306  10.271830\n",
              "  ..         ...\n",
              "  777  15.922247\n",
              "  778  16.046497\n",
              "  779  16.134884\n",
              "  780  16.227112\n",
              "  781  16.255293\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "              0\n",
              "  389  0.402265\n",
              "  390  0.403546\n",
              "  391  0.399703\n",
              "  392  0.403546\n",
              "  393  0.399703\n",
              "  ..        ...\n",
              "  864  0.395860\n",
              "  865  0.398422\n",
              "  866  0.395860\n",
              "  867  0.394579\n",
              "  868  0.398422\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "              0\n",
              "  291  1.810828\n",
              "  292  1.804423\n",
              "  293  1.803141\n",
              "  294  1.801860\n",
              "  295  1.796736\n",
              "  ..        ...\n",
              "  766  1.248226\n",
              "  767  1.252069\n",
              "  768  1.249507\n",
              "  769  1.246945\n",
              "  770  1.252069\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "              0\n",
              "  291  0.329243\n",
              "  292  0.326680\n",
              "  293  0.329243\n",
              "  294  0.326680\n",
              "  295  0.326680\n",
              "  ..        ...\n",
              "  766  0.313869\n",
              "  767  0.312588\n",
              "  768  0.312588\n",
              "  769  0.315151\n",
              "  770  0.311307\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "              0\n",
              "  293  6.700797\n",
              "  294  6.795599\n",
              "  295  6.989045\n",
              "  296  7.289968\n",
              "  297  7.505193\n",
              "  ..        ...\n",
              "  768  9.639848\n",
              "  769  9.587323\n",
              "  770  9.527111\n",
              "  771  9.468181\n",
              "  772  9.364411\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "              0\n",
              "  288  0.357427\n",
              "  289  0.354865\n",
              "  290  0.358708\n",
              "  291  0.354865\n",
              "  292  0.358708\n",
              "  ..        ...\n",
              "  763  0.431731\n",
              "  764  0.426606\n",
              "  765  0.426606\n",
              "  766  0.426606\n",
              "  767  0.429168\n",
              "  \n",
              "  [480 rows x 1 columns]],\n",
              " [             0\n",
              "  792   0.430475\n",
              "  793   0.433038\n",
              "  794   0.430475\n",
              "  795   0.431757\n",
              "  796   0.434319\n",
              "  ...        ...\n",
              "  1267  0.481722\n",
              "  1268  0.477879\n",
              "  1269  0.474035\n",
              "  1270  0.474035\n",
              "  1271  0.472754\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  782   0.171667\n",
              "  783   0.174230\n",
              "  784   0.172948\n",
              "  785   0.176792\n",
              "  786   0.172948\n",
              "  ...        ...\n",
              "  1257  0.181916\n",
              "  1258  0.184478\n",
              "  1259  0.181916\n",
              "  1260  0.181916\n",
              "  1261  0.185759\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  783   5.401949\n",
              "  784   5.422448\n",
              "  785   5.421167\n",
              "  786   5.431416\n",
              "  787   5.508287\n",
              "  ...        ...\n",
              "  1258  5.640972\n",
              "  1259  5.633286\n",
              "  1260  5.615349\n",
              "  1261  5.601256\n",
              "  1262  5.532073\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  779   0.375362\n",
              "  780   0.379206\n",
              "  781   0.376643\n",
              "  782   0.377924\n",
              "  783   0.377924\n",
              "  ...        ...\n",
              "  1254  0.521408\n",
              "  1255  0.525251\n",
              "  1256  0.522689\n",
              "  1257  0.522689\n",
              "  1258  0.523970\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "                0\n",
              "  777   23.780010\n",
              "  778   23.759514\n",
              "  779   23.872250\n",
              "  780   23.952959\n",
              "  781   23.904278\n",
              "  ...         ...\n",
              "  1252  20.034658\n",
              "  1253  20.475922\n",
              "  1254  20.387527\n",
              "  1255  20.472078\n",
              "  1256  19.339586\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  766   8.729812\n",
              "  767   8.728531\n",
              "  768   8.720844\n",
              "  769   8.718282\n",
              "  770   8.732374\n",
              "  ...        ...\n",
              "  1241  7.670538\n",
              "  1242  7.670538\n",
              "  1243  7.661571\n",
              "  1244  7.673100\n",
              "  1245  7.657727\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "                0\n",
              "  812   10.996856\n",
              "  813   10.991732\n",
              "  814   11.004542\n",
              "  815   10.987888\n",
              "  816   10.986607\n",
              "  ...         ...\n",
              "  1287  11.094902\n",
              "  1288  11.082091\n",
              "  1289  11.087216\n",
              "  1290  11.084654\n",
              "  1291  11.082091\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "                0\n",
              "  792   31.124365\n",
              "  793   31.173048\n",
              "  794   31.267849\n",
              "  795   31.256319\n",
              "  796   31.290909\n",
              "  ...         ...\n",
              "  1267  37.714481\n",
              "  1268  37.752914\n",
              "  1269  37.769569\n",
              "  1270  37.778538\n",
              "  1271  37.755478\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "                0\n",
              "  785   21.362942\n",
              "  786   21.407782\n",
              "  787   21.506426\n",
              "  788   21.582010\n",
              "  789   21.630692\n",
              "  ...         ...\n",
              "  1260  26.179190\n",
              "  1261  26.588701\n",
              "  1262  26.705954\n",
              "  1263  26.699549\n",
              "  1264  26.661116\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "                0\n",
              "  782   16.183559\n",
              "  783   16.104141\n",
              "  784   16.106703\n",
              "  785   16.050341\n",
              "  786   15.977327\n",
              "  ...         ...\n",
              "  1257  17.209108\n",
              "  1258  17.195019\n",
              "  1259  17.166838\n",
              "  1260  17.148903\n",
              "  1261  17.134813\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  869   0.394579\n",
              "  870   0.395860\n",
              "  871   0.398422\n",
              "  872   0.393298\n",
              "  873   0.398422\n",
              "  ...        ...\n",
              "  1344  0.377924\n",
              "  1345  0.375362\n",
              "  1346  0.377924\n",
              "  1347  0.375362\n",
              "  1348  0.374081\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  771   1.245664\n",
              "  772   1.244383\n",
              "  773   1.246945\n",
              "  774   1.243102\n",
              "  775   1.245664\n",
              "  ...        ...\n",
              "  1246  1.127803\n",
              "  1247  1.135489\n",
              "  1248  1.127803\n",
              "  1249  1.130365\n",
              "  1250  1.131646\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  771   0.313869\n",
              "  772   0.311307\n",
              "  773   0.313869\n",
              "  774   0.315151\n",
              "  775   0.312588\n",
              "  ...        ...\n",
              "  1246  0.374081\n",
              "  1247  0.371519\n",
              "  1248  0.375362\n",
              "  1249  0.371519\n",
              "  1250  0.375362\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "                0\n",
              "  773    9.261924\n",
              "  774    9.729525\n",
              "  775    9.787174\n",
              "  776    9.898630\n",
              "  777    9.940907\n",
              "  ...         ...\n",
              "  1248  11.842381\n",
              "  1249  11.912842\n",
              "  1250  11.896188\n",
              "  1251  11.816760\n",
              "  1252  11.709147\n",
              "  \n",
              "  [480 rows x 1 columns],\n",
              "               0\n",
              "  768   0.431731\n",
              "  769   0.429168\n",
              "  770   0.433012\n",
              "  771   0.429168\n",
              "  772   0.431731\n",
              "  ...        ...\n",
              "  1243  0.434293\n",
              "  1244  0.429168\n",
              "  1245  0.434293\n",
              "  1246  0.429168\n",
              "  1247  0.430450\n",
              "  \n",
              "  [480 rows x 1 columns]]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cI8mH_Zo1vN"
      },
      "outputs": [],
      "source": [
        "# CSV dosyasını okuma\n",
        "Features_eda_all = pd.read_csv(f'{output_path_features}/CSV/All_EDA_Features.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o-LRUGgFFNC"
      },
      "outputs": [],
      "source": [
        "type(Features_eda_all)\n",
        "EDA_baseline_DF = Features_eda_all[(Features_eda_all['Label'] == 0)].copy()\n",
        "EDA_stroop_DF   = Features_eda_all[(Features_eda_all['Label'] == 1)].copy()\n",
        "EDA_nostress_DF = Features_eda_all[(Features_eda_all['Label'] == 2)].copy()\n",
        "\n",
        "EDA_baseline_DF = EDA_baseline_DF.drop (['Participant_Id'], axis=1).copy()\n",
        "EDA_stroop_DF   = EDA_stroop_DF.drop   (['Participant_Id'], axis=1).copy()\n",
        "EDA_nostress_DF = EDA_nostress_DF.drop (['Participant_Id'], axis=1).copy()\n",
        "\n",
        "EDA_baseline_DF = EDA_baseline_DF.drop (['Label'], axis=1).copy()\n",
        "EDA_stroop_DF   = EDA_stroop_DF.drop   (['Label'], axis=1).copy()\n",
        "EDA_nostress_DF = EDA_nostress_DF.drop (['Label'], axis=1).copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toplu_dfs = [EDA_baseline,EDA_stroop,EDA_ns]\n",
        "for i in range(len(EDA_baseline)):\n",
        "  EDA_baseline[i]['Label']=0\n",
        "  EDA_stroop  [i]['Label']=1\n",
        "  EDA_ns      [i]['Label']=2\n",
        "  EDA_baseline[i]['Participant_Id']=i\n",
        "  EDA_stroop  [i]['Participant_Id']=i\n",
        "  EDA_ns      [i]['Participant_Id']=i\n",
        "  EDA_baseline[i].columns = ['Value','Label','Participant_Id']\n",
        "  EDA_stroop  [i].columns = ['Value','Label','Participant_Id']\n",
        "  EDA_ns      [i].columns = ['Value','Label','Participant_Id']\n",
        "np.shape(EDA_baseline)\n",
        "EDA_baseline[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6oUaK2NIObqk",
        "outputId": "b0cb3eff-8636-4c77-eb27-7d1f36b3bfca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Value  Label  Participant_Id\n",
              "72   0.447131      0               0\n",
              "73   0.445850      0               0\n",
              "74   0.447131      0               0\n",
              "75   0.457380      0               0\n",
              "76   0.457380      0               0\n",
              "..        ...    ...             ...\n",
              "307  0.461224      0               0\n",
              "308  0.457380      0               0\n",
              "309  0.459942      0               0\n",
              "310  0.457380      0               0\n",
              "311  0.458661      0               0\n",
              "\n",
              "[240 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a430b47-8c67-4d8b-a597-2c5ae38c2810\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "      <th>Label</th>\n",
              "      <th>Participant_Id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.447131</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.445850</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.447131</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.457380</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.457380</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>0.461224</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>0.457380</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>0.459942</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>0.457380</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>0.458661</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>240 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a430b47-8c67-4d8b-a597-2c5ae38c2810')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a430b47-8c67-4d8b-a597-2c5ae38c2810 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a430b47-8c67-4d8b-a597-2c5ae38c2810');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d948c83f-271c-4958-bac7-5c9cdf3ae243\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d948c83f-271c-4958-bac7-5c9cdf3ae243')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d948c83f-271c-4958-bac7-5c9cdf3ae243 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soxtqOMxedM1"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWSCZlUWijcS"
      },
      "outputs": [],
      "source": [
        "# Concatenate along a new axis (axis=1 in this example)\n",
        "eda_total_df = np.concatenate([EDA_baseline, EDA_stroop, EDA_ns], axis=1)\n",
        "\n",
        "# Create a DataFrame from the concatenated array\n",
        "eda_total_df = pd.DataFrame(eda_total_df.reshape(-1, eda_total_df.shape[-1]))\n",
        "\n",
        "# Resetting index after concatenation\n",
        "eda_total_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "eda_labels = eda_total_df[1].tolist()\n",
        "eda_participants = eda_total_df[2].tolist()\n",
        "eda_total_df = eda_total_df.drop(columns=[1, 2])\n",
        "#0 = DEGER 1 = LABEL 2=PARTICIPANT\n",
        "\n",
        "Features_eda_all\n",
        "df = pd.DataFrame(Features_eda_all)\n",
        "df = df.drop('Participant_Id', axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eda_raw= eda_total_df\n",
        "eda_raw_label = eda_labels\n",
        "df = pd.DataFrame(eda_raw_label)\n",
        "df1 = pd.DataFrame(eda_raw)\n",
        "\n",
        "df1.to_csv(f'{output_path_features}/CSV/eda_raw.csv', index=False)  # index=False, indeks sütununu CSV'ye yazmamak içindf = pd.DataFrame(liste)\n",
        "\n",
        "df.to_csv(f'{output_path_features}/CSV/eda_labels.csv', index=False)  # index=False, indeks sütununu CSV'ye yazmamak için\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "zU-CGTE4AWb2",
        "outputId": "bd2591fb-7e9c-4f63-d909-6896eabe436f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0      0.0\n",
              "1      0.0\n",
              "2      0.0\n",
              "3      0.0\n",
              "4      0.0\n",
              "...    ...\n",
              "17995  2.0\n",
              "17996  2.0\n",
              "17997  2.0\n",
              "17998  2.0\n",
              "17999  2.0\n",
              "\n",
              "[18000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9ba80b9-5df7-40f5-a8a4-a1c6bd9f71ce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17995</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17996</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17997</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17998</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17999</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18000 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9ba80b9-5df7-40f5-a8a4-a1c6bd9f71ce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d9ba80b9-5df7-40f5-a8a4-a1c6bd9f71ce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d9ba80b9-5df7-40f5-a8a4-a1c6bd9f71ce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d5b435fa-278b-4f50-905f-003db9340cc4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d5b435fa-278b-4f50-905f-003db9340cc4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d5b435fa-278b-4f50-905f-003db9340cc4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8abad54a-4ca8-493e-a38b-5c7731977474\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8abad54a-4ca8-493e-a38b-5c7731977474 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cevQkrLRfbX_"
      },
      "outputs": [],
      "source": [
        "# Fix random seed for reproducibility\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "# Normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "eda_total_df_scaled = scaler.fit_transform(eda_total_df)\n",
        "dataset=eda_total_df_scaled"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_binary=df['Label']\n",
        "y=df['Label']\n",
        "\n",
        "# 'Label' sütununda 2 olan değerleri 0 ile değiştirelim\n",
        "y_binary.loc[y_binary == 2] = 0\n",
        "y_binary = to_categorical(y_binary)\n",
        "y_binary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWuN4-Ogmil_",
        "outputId": "0366174f-e9a3-41df-fc4a-845fb6ec903f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       ...,\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix to feed into the LSTM model\n",
        "X = df.drop('Label', axis =1).values\n",
        "y = to_categorical(df['Label'])\n",
        "\n",
        "# Scale the data between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Separate the data into a 75% train / 25% test split.\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size=0.25, random_state=36)\n",
        "# Reshape the input variables to fit the expected input shape\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
      ],
      "metadata": {
        "id": "pzAn8KNpQhAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP1b8fd_juEj"
      },
      "outputs": [],
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        " dataX, dataY = [], []\n",
        " for i in range(len(dataset)-look_back-1):\n",
        "  a = dataset[i:(i+look_back), 0]\n",
        "  dataX.append(a)\n",
        "  dataY.append(dataset[i + look_back, 0])\n",
        " return np.array(dataX), np.array(dataY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best MODEL"
      ],
      "metadata": {
        "id": "kAA4ooYxN88h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YyjagDVcctN",
        "outputId": "1c2bf332-22cb-478f-8fb3-009e9a51451a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3637, 1, 61)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There will be 15 neurons in the LSTM network, one for each input.\n",
        "num_neurons = 15\n",
        "num_features = 61\n",
        "\n",
        "def get_new_model():\n",
        "\n",
        "    # Create the model instance and define the network structure\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=True))\n",
        "    # Consider removing the Flatten() layer to maintain sequential information\n",
        "\n",
        "    # Add Batch Normalization and Dropout between layers\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(num_neurons, activation='relu'))\n",
        "\n",
        "    # Adjust the input shape for the second LSTM or other layers\n",
        "    model.add(LSTM(num_neurons, activation=\"relu\", return_sequences=False))\n",
        "    # Add Batch Normalization and Dropout between layers\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(num_neurons, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xrPTQT6O2eHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJuqyUBg_jQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db677a2-c23f-4a31-9479-3336cf968785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM...\n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.010000\n",
            "\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 10s 8ms/step - loss: 0.6218 - accuracy: 0.4550 - val_loss: 0.5848 - val_accuracy: 0.5045\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5894 - accuracy: 0.4911 - val_loss: 0.5689 - val_accuracy: 0.5309\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5840 - accuracy: 0.4853 - val_loss: 0.5697 - val_accuracy: 0.5194\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5777 - accuracy: 0.5150 - val_loss: 0.5616 - val_accuracy: 0.5045\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5750 - accuracy: 0.5029 - val_loss: 0.5597 - val_accuracy: 0.5359\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5727 - accuracy: 0.5205 - val_loss: 0.5617 - val_accuracy: 0.5095\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5677 - accuracy: 0.5238 - val_loss: 0.5603 - val_accuracy: 0.5210\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5698 - accuracy: 0.5186 - val_loss: 0.5519 - val_accuracy: 0.5664\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5660 - accuracy: 0.5345 - val_loss: 0.5472 - val_accuracy: 0.5507\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5676 - accuracy: 0.5276 - val_loss: 0.5495 - val_accuracy: 0.5515\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5631 - accuracy: 0.5320 - val_loss: 0.5461 - val_accuracy: 0.5458\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5618 - accuracy: 0.5293 - val_loss: 0.5517 - val_accuracy: 0.5474\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5652 - accuracy: 0.5301 - val_loss: 0.5427 - val_accuracy: 0.5738\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5611 - accuracy: 0.5392 - val_loss: 0.5426 - val_accuracy: 0.5688\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5605 - accuracy: 0.5375 - val_loss: 0.5463 - val_accuracy: 0.5664\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5610 - accuracy: 0.5419 - val_loss: 0.5369 - val_accuracy: 0.5936\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5554 - accuracy: 0.5510 - val_loss: 0.5381 - val_accuracy: 0.5680\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 4s 6ms/step - loss: 0.5542 - accuracy: 0.5543 - val_loss: 0.5312 - val_accuracy: 0.5977\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5524 - accuracy: 0.5513 - val_loss: 0.5270 - val_accuracy: 0.5812\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5560 - accuracy: 0.5461 - val_loss: 0.5320 - val_accuracy: 0.5655\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5550 - accuracy: 0.5403 - val_loss: 0.5438 - val_accuracy: 0.5169\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5530 - accuracy: 0.5491 - val_loss: 0.5614 - val_accuracy: 0.4905\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5517 - accuracy: 0.5535 - val_loss: 0.5447 - val_accuracy: 0.5458\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5517 - accuracy: 0.5543 - val_loss: 0.5389 - val_accuracy: 0.5614\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 4s 6ms/step - loss: 0.5536 - accuracy: 0.5557 - val_loss: 0.5183 - val_accuracy: 0.5870\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5513 - accuracy: 0.5463 - val_loss: 0.5302 - val_accuracy: 0.5647\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5491 - accuracy: 0.5532 - val_loss: 0.5229 - val_accuracy: 0.5787\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5465 - accuracy: 0.5595 - val_loss: 0.5189 - val_accuracy: 0.5812\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5464 - accuracy: 0.5568 - val_loss: 0.5198 - val_accuracy: 0.6076\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5467 - accuracy: 0.5430 - val_loss: 0.5197 - val_accuracy: 0.5878\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 4s 6ms/step - loss: 0.5498 - accuracy: 0.5510 - val_loss: 0.5274 - val_accuracy: 0.5631\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5483 - accuracy: 0.5549 - val_loss: 0.5250 - val_accuracy: 0.5763\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5447 - accuracy: 0.5623 - val_loss: 0.5187 - val_accuracy: 0.5911\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5391 - accuracy: 0.5659 - val_loss: 0.5137 - val_accuracy: 0.5985\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5449 - accuracy: 0.5645 - val_loss: 0.5169 - val_accuracy: 0.5853\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5470 - accuracy: 0.5483 - val_loss: 0.5191 - val_accuracy: 0.5779\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5434 - accuracy: 0.5538 - val_loss: 0.5114 - val_accuracy: 0.5878\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5435 - accuracy: 0.5604 - val_loss: 0.5358 - val_accuracy: 0.5482\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5388 - accuracy: 0.5689 - val_loss: 0.5149 - val_accuracy: 0.5969\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5421 - accuracy: 0.5650 - val_loss: 0.5106 - val_accuracy: 0.5969\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5381 - accuracy: 0.5661 - val_loss: 0.5093 - val_accuracy: 0.5853\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5400 - accuracy: 0.5631 - val_loss: 0.5279 - val_accuracy: 0.5474\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5454 - accuracy: 0.5571 - val_loss: 0.5118 - val_accuracy: 0.5936\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5412 - accuracy: 0.5678 - val_loss: 0.5100 - val_accuracy: 0.5993\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5361 - accuracy: 0.5766 - val_loss: 0.5076 - val_accuracy: 0.6117\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5402 - accuracy: 0.5637 - val_loss: 0.5083 - val_accuracy: 0.6018\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5368 - accuracy: 0.5642 - val_loss: 0.5188 - val_accuracy: 0.5903\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5342 - accuracy: 0.5700 - val_loss: 0.5058 - val_accuracy: 0.6002\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5356 - accuracy: 0.5735 - val_loss: 0.5339 - val_accuracy: 0.5458\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5349 - accuracy: 0.5774 - val_loss: 0.5030 - val_accuracy: 0.6092\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5377 - accuracy: 0.5763 - val_loss: 0.5015 - val_accuracy: 0.6150\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5357 - accuracy: 0.5653 - val_loss: 0.4997 - val_accuracy: 0.5993\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5397 - accuracy: 0.5601 - val_loss: 0.5091 - val_accuracy: 0.6018\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5333 - accuracy: 0.5793 - val_loss: 0.4990 - val_accuracy: 0.5878\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5349 - accuracy: 0.5738 - val_loss: 0.5012 - val_accuracy: 0.6257\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5346 - accuracy: 0.5645 - val_loss: 0.4910 - val_accuracy: 0.6274\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5293 - accuracy: 0.5815 - val_loss: 0.5010 - val_accuracy: 0.5878\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5323 - accuracy: 0.5716 - val_loss: 0.5210 - val_accuracy: 0.5721\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5319 - accuracy: 0.5755 - val_loss: 0.4890 - val_accuracy: 0.6158\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5348 - accuracy: 0.5656 - val_loss: 0.4955 - val_accuracy: 0.6035\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5317 - accuracy: 0.5656 - val_loss: 0.4964 - val_accuracy: 0.6125\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5258 - accuracy: 0.5832 - val_loss: 0.4953 - val_accuracy: 0.6002\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5293 - accuracy: 0.5694 - val_loss: 0.4968 - val_accuracy: 0.6092\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5268 - accuracy: 0.5815 - val_loss: 0.4955 - val_accuracy: 0.6084\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5273 - accuracy: 0.5760 - val_loss: 0.4959 - val_accuracy: 0.5919\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 4s 6ms/step - loss: 0.5314 - accuracy: 0.5711 - val_loss: 0.4933 - val_accuracy: 0.5886\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5224 - accuracy: 0.5799 - val_loss: 0.4918 - val_accuracy: 0.6092\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5303 - accuracy: 0.5733 - val_loss: 0.5056 - val_accuracy: 0.5754\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5327 - accuracy: 0.5793 - val_loss: 0.4901 - val_accuracy: 0.6125\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5246 - accuracy: 0.5777 - val_loss: 0.4866 - val_accuracy: 0.6142\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5310 - accuracy: 0.5722 - val_loss: 0.4989 - val_accuracy: 0.5845\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5259 - accuracy: 0.5681 - val_loss: 0.4843 - val_accuracy: 0.6307\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5301 - accuracy: 0.5744 - val_loss: 0.4990 - val_accuracy: 0.6059\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5266 - accuracy: 0.5670 - val_loss: 0.4969 - val_accuracy: 0.6092\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.5268 - accuracy: 0.5763 - val_loss: 0.4990 - val_accuracy: 0.5787\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5250 - accuracy: 0.5799 - val_loss: 0.4780 - val_accuracy: 0.6373\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5268 - accuracy: 0.5807 - val_loss: 0.4973 - val_accuracy: 0.5911\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5235 - accuracy: 0.5851 - val_loss: 0.4933 - val_accuracy: 0.5746\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5182 - accuracy: 0.5966 - val_loss: 0.5001 - val_accuracy: 0.5878\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5266 - accuracy: 0.5733 - val_loss: 0.4892 - val_accuracy: 0.6101\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5224 - accuracy: 0.5785 - val_loss: 0.4828 - val_accuracy: 0.6018\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5193 - accuracy: 0.5856 - val_loss: 0.4900 - val_accuracy: 0.6364\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5231 - accuracy: 0.5790 - val_loss: 0.4696 - val_accuracy: 0.6265\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5204 - accuracy: 0.5854 - val_loss: 0.4764 - val_accuracy: 0.6348\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5209 - accuracy: 0.5898 - val_loss: 0.4747 - val_accuracy: 0.6381\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5195 - accuracy: 0.6038 - val_loss: 0.4792 - val_accuracy: 0.6414\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5224 - accuracy: 0.5840 - val_loss: 0.4762 - val_accuracy: 0.6150\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5167 - accuracy: 0.5818 - val_loss: 0.4740 - val_accuracy: 0.6521\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5134 - accuracy: 0.6002 - val_loss: 0.5025 - val_accuracy: 0.6043\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5183 - accuracy: 0.5911 - val_loss: 0.4700 - val_accuracy: 0.6406\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5228 - accuracy: 0.5887 - val_loss: 0.4668 - val_accuracy: 0.6224\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5192 - accuracy: 0.5909 - val_loss: 0.5193 - val_accuracy: 0.5540\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5190 - accuracy: 0.5873 - val_loss: 0.4596 - val_accuracy: 0.6480\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.5172 - accuracy: 0.5898 - val_loss: 0.4820 - val_accuracy: 0.6331\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5162 - accuracy: 0.5856 - val_loss: 0.4711 - val_accuracy: 0.6397\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5177 - accuracy: 0.5865 - val_loss: 0.4680 - val_accuracy: 0.6381\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5186 - accuracy: 0.5881 - val_loss: 0.5011 - val_accuracy: 0.6092\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5162 - accuracy: 0.5994 - val_loss: 0.4875 - val_accuracy: 0.6298\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5119 - accuracy: 0.6041 - val_loss: 0.4633 - val_accuracy: 0.6538\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5095 - accuracy: 0.6060 - val_loss: 0.4705 - val_accuracy: 0.6331\n",
            "243/243 [==============================] - 1s 3ms/step - loss: 0.4705 - accuracy: 0.6331\n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.020000\n",
            "\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 11s 10ms/step - loss: 0.6132 - accuracy: 0.4553 - val_loss: 0.5720 - val_accuracy: 0.4963\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5829 - accuracy: 0.4957 - val_loss: 0.5637 - val_accuracy: 0.5482\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5787 - accuracy: 0.5128 - val_loss: 0.5606 - val_accuracy: 0.5540\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5700 - accuracy: 0.5257 - val_loss: 0.5555 - val_accuracy: 0.5474\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5709 - accuracy: 0.5243 - val_loss: 0.5528 - val_accuracy: 0.5688\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5673 - accuracy: 0.5367 - val_loss: 0.5504 - val_accuracy: 0.5721\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5645 - accuracy: 0.5384 - val_loss: 0.5539 - val_accuracy: 0.5045\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5655 - accuracy: 0.5384 - val_loss: 0.5499 - val_accuracy: 0.5655\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5635 - accuracy: 0.5373 - val_loss: 0.5439 - val_accuracy: 0.5779\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5586 - accuracy: 0.5455 - val_loss: 0.5418 - val_accuracy: 0.5746\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5606 - accuracy: 0.5472 - val_loss: 0.5335 - val_accuracy: 0.5820\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5548 - accuracy: 0.5546 - val_loss: 0.5371 - val_accuracy: 0.5713\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5595 - accuracy: 0.5351 - val_loss: 0.5368 - val_accuracy: 0.5655\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5553 - accuracy: 0.5463 - val_loss: 0.5420 - val_accuracy: 0.5581\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5557 - accuracy: 0.5524 - val_loss: 0.5388 - val_accuracy: 0.5754\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5533 - accuracy: 0.5521 - val_loss: 0.5295 - val_accuracy: 0.5985\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5548 - accuracy: 0.5455 - val_loss: 0.5783 - val_accuracy: 0.5004\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5511 - accuracy: 0.5549 - val_loss: 0.5337 - val_accuracy: 0.5853\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5483 - accuracy: 0.5546 - val_loss: 0.5267 - val_accuracy: 0.5812\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5461 - accuracy: 0.5587 - val_loss: 0.5266 - val_accuracy: 0.6175\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5475 - accuracy: 0.5516 - val_loss: 0.6364 - val_accuracy: 0.4988\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5514 - accuracy: 0.5604 - val_loss: 0.5302 - val_accuracy: 0.5878\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5431 - accuracy: 0.5670 - val_loss: 0.5281 - val_accuracy: 0.5870\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5474 - accuracy: 0.5549 - val_loss: 0.5227 - val_accuracy: 0.5977\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5494 - accuracy: 0.5584 - val_loss: 0.5165 - val_accuracy: 0.6018\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5428 - accuracy: 0.5670 - val_loss: 0.5165 - val_accuracy: 0.6010\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5434 - accuracy: 0.5634 - val_loss: 0.5103 - val_accuracy: 0.6018\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5403 - accuracy: 0.5642 - val_loss: 0.5121 - val_accuracy: 0.6018\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5460 - accuracy: 0.5612 - val_loss: 0.5097 - val_accuracy: 0.6084\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5412 - accuracy: 0.5670 - val_loss: 0.5190 - val_accuracy: 0.5820\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5426 - accuracy: 0.5631 - val_loss: 0.5160 - val_accuracy: 0.5919\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5391 - accuracy: 0.5664 - val_loss: 0.5098 - val_accuracy: 0.6010\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5399 - accuracy: 0.5642 - val_loss: 0.5054 - val_accuracy: 0.6051\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5368 - accuracy: 0.5590 - val_loss: 0.5018 - val_accuracy: 0.6043\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5357 - accuracy: 0.5757 - val_loss: 0.5194 - val_accuracy: 0.5812\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5411 - accuracy: 0.5667 - val_loss: 0.5064 - val_accuracy: 0.6043\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5352 - accuracy: 0.5799 - val_loss: 0.4971 - val_accuracy: 0.6092\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5354 - accuracy: 0.5716 - val_loss: 0.5175 - val_accuracy: 0.5664\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5300 - accuracy: 0.5760 - val_loss: 0.5092 - val_accuracy: 0.5779\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5358 - accuracy: 0.5719 - val_loss: 0.4889 - val_accuracy: 0.6323\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5351 - accuracy: 0.5584 - val_loss: 0.4971 - val_accuracy: 0.6241\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5339 - accuracy: 0.5700 - val_loss: 0.5041 - val_accuracy: 0.5730\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5326 - accuracy: 0.5799 - val_loss: 0.5012 - val_accuracy: 0.6076\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5306 - accuracy: 0.5796 - val_loss: 0.4907 - val_accuracy: 0.6257\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5315 - accuracy: 0.5878 - val_loss: 0.4931 - val_accuracy: 0.6241\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5267 - accuracy: 0.5851 - val_loss: 0.4931 - val_accuracy: 0.6191\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5340 - accuracy: 0.5711 - val_loss: 0.5033 - val_accuracy: 0.5985\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5258 - accuracy: 0.5873 - val_loss: 0.4890 - val_accuracy: 0.6274\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5304 - accuracy: 0.5829 - val_loss: 0.5003 - val_accuracy: 0.6010\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5234 - accuracy: 0.5920 - val_loss: 0.5041 - val_accuracy: 0.5993\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5277 - accuracy: 0.5804 - val_loss: 0.4883 - val_accuracy: 0.6315\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5282 - accuracy: 0.5807 - val_loss: 0.4920 - val_accuracy: 0.6076\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5310 - accuracy: 0.5703 - val_loss: 0.5088 - val_accuracy: 0.5804\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5228 - accuracy: 0.5931 - val_loss: 0.4916 - val_accuracy: 0.6134\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5220 - accuracy: 0.5881 - val_loss: 0.4852 - val_accuracy: 0.6282\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5276 - accuracy: 0.5744 - val_loss: 0.4955 - val_accuracy: 0.6183\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5206 - accuracy: 0.5837 - val_loss: 0.4748 - val_accuracy: 0.6307\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5230 - accuracy: 0.5810 - val_loss: 0.4770 - val_accuracy: 0.6224\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5187 - accuracy: 0.5909 - val_loss: 0.4861 - val_accuracy: 0.5894\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5241 - accuracy: 0.5878 - val_loss: 0.4823 - val_accuracy: 0.6150\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5187 - accuracy: 0.5832 - val_loss: 0.4820 - val_accuracy: 0.6026\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5155 - accuracy: 0.5914 - val_loss: 0.4897 - val_accuracy: 0.6282\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5197 - accuracy: 0.5928 - val_loss: 0.4721 - val_accuracy: 0.6496\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5174 - accuracy: 0.5856 - val_loss: 0.4827 - val_accuracy: 0.6241\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5208 - accuracy: 0.5867 - val_loss: 0.4704 - val_accuracy: 0.6472\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5209 - accuracy: 0.5763 - val_loss: 0.4691 - val_accuracy: 0.6505\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5160 - accuracy: 0.6008 - val_loss: 0.4736 - val_accuracy: 0.6200\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5096 - accuracy: 0.6002 - val_loss: 0.5023 - val_accuracy: 0.5870\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5182 - accuracy: 0.5884 - val_loss: 0.4591 - val_accuracy: 0.6587\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5122 - accuracy: 0.6010 - val_loss: 0.4955 - val_accuracy: 0.6249\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5119 - accuracy: 0.5939 - val_loss: 0.4707 - val_accuracy: 0.6554\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5228 - accuracy: 0.5766 - val_loss: 0.4727 - val_accuracy: 0.6529\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5127 - accuracy: 0.5900 - val_loss: 0.4815 - val_accuracy: 0.6265\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5164 - accuracy: 0.5889 - val_loss: 0.4642 - val_accuracy: 0.6290\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5123 - accuracy: 0.5889 - val_loss: 0.4717 - val_accuracy: 0.6455\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5096 - accuracy: 0.5991 - val_loss: 0.4482 - val_accuracy: 0.6645\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5132 - accuracy: 0.5900 - val_loss: 0.4613 - val_accuracy: 0.6323\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5033 - accuracy: 0.6098 - val_loss: 0.4614 - val_accuracy: 0.6463\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5053 - accuracy: 0.6068 - val_loss: 0.4522 - val_accuracy: 0.6669\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5120 - accuracy: 0.5920 - val_loss: 0.4667 - val_accuracy: 0.6109\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5062 - accuracy: 0.5958 - val_loss: 0.4494 - val_accuracy: 0.6678\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5079 - accuracy: 0.5922 - val_loss: 0.4601 - val_accuracy: 0.6669\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5074 - accuracy: 0.5955 - val_loss: 0.5038 - val_accuracy: 0.6158\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 5s 6ms/step - loss: 0.5051 - accuracy: 0.6060 - val_loss: 0.4711 - val_accuracy: 0.6076\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5029 - accuracy: 0.6013 - val_loss: 0.4527 - val_accuracy: 0.6612\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5122 - accuracy: 0.5972 - val_loss: 0.4551 - val_accuracy: 0.6735\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5074 - accuracy: 0.5906 - val_loss: 0.4505 - val_accuracy: 0.6678\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5020 - accuracy: 0.6104 - val_loss: 0.4763 - val_accuracy: 0.6389\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4977 - accuracy: 0.6101 - val_loss: 0.4559 - val_accuracy: 0.6315\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5018 - accuracy: 0.6068 - val_loss: 0.4711 - val_accuracy: 0.6447\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5062 - accuracy: 0.6038 - val_loss: 0.4823 - val_accuracy: 0.6134\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4994 - accuracy: 0.6090 - val_loss: 0.4554 - val_accuracy: 0.6570\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5022 - accuracy: 0.6087 - val_loss: 0.4824 - val_accuracy: 0.5927\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4977 - accuracy: 0.6071 - val_loss: 0.4546 - val_accuracy: 0.6653\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4985 - accuracy: 0.6118 - val_loss: 0.4900 - val_accuracy: 0.6538\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4969 - accuracy: 0.6104 - val_loss: 0.4574 - val_accuracy: 0.6636\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5073 - accuracy: 0.5972 - val_loss: 0.4831 - val_accuracy: 0.6331\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.4970 - accuracy: 0.6052 - val_loss: 0.5101 - val_accuracy: 0.5903\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4963 - accuracy: 0.6134 - val_loss: 0.4726 - val_accuracy: 0.6480\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4980 - accuracy: 0.6096 - val_loss: 0.4672 - val_accuracy: 0.6463\n",
            "243/243 [==============================] - 1s 3ms/step - loss: 0.4672 - accuracy: 0.6463\n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.030000\n",
            "\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 11s 10ms/step - loss: 0.6043 - accuracy: 0.4740 - val_loss: 0.5787 - val_accuracy: 0.4963\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5884 - accuracy: 0.4858 - val_loss: 0.5670 - val_accuracy: 0.5392\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5771 - accuracy: 0.5048 - val_loss: 0.5616 - val_accuracy: 0.5359\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5723 - accuracy: 0.5243 - val_loss: 0.5534 - val_accuracy: 0.5639\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5748 - accuracy: 0.5081 - val_loss: 0.5511 - val_accuracy: 0.5622\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5671 - accuracy: 0.5331 - val_loss: 0.5622 - val_accuracy: 0.5070\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5675 - accuracy: 0.5309 - val_loss: 0.5466 - val_accuracy: 0.5120\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5660 - accuracy: 0.5309 - val_loss: 0.5466 - val_accuracy: 0.5837\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5624 - accuracy: 0.5378 - val_loss: 0.5336 - val_accuracy: 0.5829\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5584 - accuracy: 0.5408 - val_loss: 0.5351 - val_accuracy: 0.5779\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5553 - accuracy: 0.5480 - val_loss: 0.5298 - val_accuracy: 0.5721\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5495 - accuracy: 0.5488 - val_loss: 0.5391 - val_accuracy: 0.5894\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5545 - accuracy: 0.5441 - val_loss: 0.5266 - val_accuracy: 0.5845\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5515 - accuracy: 0.5436 - val_loss: 0.5406 - val_accuracy: 0.5474\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5543 - accuracy: 0.5452 - val_loss: 0.5226 - val_accuracy: 0.5936\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5487 - accuracy: 0.5513 - val_loss: 0.5156 - val_accuracy: 0.5763\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5524 - accuracy: 0.5483 - val_loss: 0.5238 - val_accuracy: 0.5705\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5473 - accuracy: 0.5565 - val_loss: 0.5363 - val_accuracy: 0.5886\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5446 - accuracy: 0.5551 - val_loss: 0.5206 - val_accuracy: 0.5837\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5470 - accuracy: 0.5598 - val_loss: 0.5191 - val_accuracy: 0.5713\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5449 - accuracy: 0.5604 - val_loss: 0.5102 - val_accuracy: 0.6010\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5447 - accuracy: 0.5516 - val_loss: 0.5318 - val_accuracy: 0.5523\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5375 - accuracy: 0.5686 - val_loss: 0.5322 - val_accuracy: 0.5334\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5457 - accuracy: 0.5494 - val_loss: 0.5153 - val_accuracy: 0.5804\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5395 - accuracy: 0.5733 - val_loss: 0.5165 - val_accuracy: 0.5853\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5402 - accuracy: 0.5642 - val_loss: 0.5120 - val_accuracy: 0.5862\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5430 - accuracy: 0.5590 - val_loss: 0.5123 - val_accuracy: 0.6068\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5360 - accuracy: 0.5771 - val_loss: 0.5120 - val_accuracy: 0.6109\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5409 - accuracy: 0.5584 - val_loss: 0.5333 - val_accuracy: 0.5820\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5386 - accuracy: 0.5689 - val_loss: 0.5505 - val_accuracy: 0.5383\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5397 - accuracy: 0.5562 - val_loss: 0.5117 - val_accuracy: 0.5771\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5356 - accuracy: 0.5650 - val_loss: 0.5091 - val_accuracy: 0.5845\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5347 - accuracy: 0.5601 - val_loss: 0.5144 - val_accuracy: 0.6109\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5378 - accuracy: 0.5612 - val_loss: 0.4955 - val_accuracy: 0.6150\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5353 - accuracy: 0.5681 - val_loss: 0.5120 - val_accuracy: 0.5936\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5370 - accuracy: 0.5543 - val_loss: 0.5050 - val_accuracy: 0.6158\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5309 - accuracy: 0.5818 - val_loss: 0.4940 - val_accuracy: 0.6472\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5330 - accuracy: 0.5697 - val_loss: 0.5146 - val_accuracy: 0.5952\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5292 - accuracy: 0.5714 - val_loss: 0.5048 - val_accuracy: 0.6175\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5369 - accuracy: 0.5716 - val_loss: 0.5219 - val_accuracy: 0.5837\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5326 - accuracy: 0.5653 - val_loss: 0.4951 - val_accuracy: 0.6026\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5290 - accuracy: 0.5735 - val_loss: 0.4987 - val_accuracy: 0.5952\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5338 - accuracy: 0.5719 - val_loss: 0.5110 - val_accuracy: 0.6191\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5276 - accuracy: 0.5790 - val_loss: 0.5124 - val_accuracy: 0.5977\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5288 - accuracy: 0.5763 - val_loss: 0.5169 - val_accuracy: 0.5787\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5241 - accuracy: 0.5763 - val_loss: 0.4938 - val_accuracy: 0.6158\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5305 - accuracy: 0.5818 - val_loss: 0.5011 - val_accuracy: 0.6307\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5221 - accuracy: 0.5815 - val_loss: 0.4883 - val_accuracy: 0.6125\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5257 - accuracy: 0.5746 - val_loss: 0.5055 - val_accuracy: 0.5680\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5235 - accuracy: 0.5862 - val_loss: 0.5156 - val_accuracy: 0.5936\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5241 - accuracy: 0.5867 - val_loss: 0.4855 - val_accuracy: 0.6348\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5225 - accuracy: 0.5903 - val_loss: 0.4905 - val_accuracy: 0.6290\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5227 - accuracy: 0.5807 - val_loss: 0.5217 - val_accuracy: 0.5936\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5163 - accuracy: 0.5969 - val_loss: 0.4882 - val_accuracy: 0.6265\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5268 - accuracy: 0.5716 - val_loss: 0.4927 - val_accuracy: 0.6364\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5187 - accuracy: 0.5944 - val_loss: 0.4781 - val_accuracy: 0.6191\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5184 - accuracy: 0.5766 - val_loss: 0.4884 - val_accuracy: 0.6208\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5196 - accuracy: 0.5845 - val_loss: 0.5060 - val_accuracy: 0.5952\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5180 - accuracy: 0.5840 - val_loss: 0.4829 - val_accuracy: 0.6686\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5243 - accuracy: 0.5837 - val_loss: 0.5043 - val_accuracy: 0.6076\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5128 - accuracy: 0.5854 - val_loss: 0.4975 - val_accuracy: 0.6026\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5184 - accuracy: 0.5766 - val_loss: 0.5090 - val_accuracy: 0.6200\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5178 - accuracy: 0.5878 - val_loss: 0.5127 - val_accuracy: 0.5598\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5174 - accuracy: 0.5749 - val_loss: 0.5014 - val_accuracy: 0.5705\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5112 - accuracy: 0.5903 - val_loss: 0.5116 - val_accuracy: 0.6134\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5175 - accuracy: 0.5840 - val_loss: 0.4969 - val_accuracy: 0.6175\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5174 - accuracy: 0.5876 - val_loss: 0.4816 - val_accuracy: 0.6447\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5124 - accuracy: 0.5944 - val_loss: 0.5047 - val_accuracy: 0.6232\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5152 - accuracy: 0.5840 - val_loss: 0.4712 - val_accuracy: 0.6513\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5109 - accuracy: 0.5977 - val_loss: 0.4785 - val_accuracy: 0.6348\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5077 - accuracy: 0.5980 - val_loss: 0.5074 - val_accuracy: 0.6059\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5153 - accuracy: 0.5810 - val_loss: 0.4933 - val_accuracy: 0.6109\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5092 - accuracy: 0.5958 - val_loss: 0.4776 - val_accuracy: 0.6570\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5092 - accuracy: 0.5898 - val_loss: 0.4968 - val_accuracy: 0.5870\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5134 - accuracy: 0.5856 - val_loss: 0.4840 - val_accuracy: 0.6348\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5080 - accuracy: 0.5961 - val_loss: 0.4908 - val_accuracy: 0.6340\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5074 - accuracy: 0.5867 - val_loss: 0.4838 - val_accuracy: 0.6488\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5057 - accuracy: 0.6019 - val_loss: 0.5016 - val_accuracy: 0.6257\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5039 - accuracy: 0.6046 - val_loss: 0.4657 - val_accuracy: 0.6513\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5044 - accuracy: 0.6087 - val_loss: 0.4801 - val_accuracy: 0.6109\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4997 - accuracy: 0.6046 - val_loss: 0.4802 - val_accuracy: 0.6628\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4983 - accuracy: 0.5975 - val_loss: 0.4947 - val_accuracy: 0.6422\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5049 - accuracy: 0.5966 - val_loss: 0.4799 - val_accuracy: 0.6315\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5057 - accuracy: 0.5969 - val_loss: 0.4636 - val_accuracy: 0.6546\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5034 - accuracy: 0.6085 - val_loss: 0.4867 - val_accuracy: 0.6257\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5000 - accuracy: 0.6120 - val_loss: 0.5086 - val_accuracy: 0.5845\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5028 - accuracy: 0.6046 - val_loss: 0.5078 - val_accuracy: 0.6513\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5030 - accuracy: 0.5988 - val_loss: 0.4645 - val_accuracy: 0.6678\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4981 - accuracy: 0.6082 - val_loss: 0.4521 - val_accuracy: 0.6669\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5036 - accuracy: 0.5997 - val_loss: 0.5461 - val_accuracy: 0.5977\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4987 - accuracy: 0.6013 - val_loss: 0.4813 - val_accuracy: 0.6686\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4967 - accuracy: 0.6085 - val_loss: 0.4939 - val_accuracy: 0.6529\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4977 - accuracy: 0.6049 - val_loss: 0.4685 - val_accuracy: 0.6538\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4918 - accuracy: 0.6104 - val_loss: 0.5205 - val_accuracy: 0.6232\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5017 - accuracy: 0.6079 - val_loss: 0.4643 - val_accuracy: 0.6389\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4986 - accuracy: 0.5980 - val_loss: 0.5111 - val_accuracy: 0.6035\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4975 - accuracy: 0.6065 - val_loss: 0.5422 - val_accuracy: 0.6043\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4960 - accuracy: 0.6173 - val_loss: 0.5011 - val_accuracy: 0.6224\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4936 - accuracy: 0.6071 - val_loss: 0.4680 - val_accuracy: 0.6290\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4916 - accuracy: 0.6153 - val_loss: 0.4641 - val_accuracy: 0.6595\n",
            "243/243 [==============================] - 1s 4ms/step - loss: 0.4641 - accuracy: 0.6595\n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.040000\n",
            "\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 11s 10ms/step - loss: 0.5988 - accuracy: 0.4746 - val_loss: 0.5768 - val_accuracy: 0.5021\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5777 - accuracy: 0.5029 - val_loss: 0.5645 - val_accuracy: 0.5392\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5719 - accuracy: 0.5114 - val_loss: 0.5624 - val_accuracy: 0.5598\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5661 - accuracy: 0.5199 - val_loss: 0.5544 - val_accuracy: 0.5573\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5679 - accuracy: 0.5263 - val_loss: 0.5565 - val_accuracy: 0.5655\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5618 - accuracy: 0.5274 - val_loss: 0.5564 - val_accuracy: 0.5408\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5612 - accuracy: 0.5356 - val_loss: 0.5547 - val_accuracy: 0.4930\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5626 - accuracy: 0.5309 - val_loss: 0.5531 - val_accuracy: 0.5334\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.5589 - accuracy: 0.5417 - val_loss: 0.5439 - val_accuracy: 0.5606\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5560 - accuracy: 0.5458 - val_loss: 0.5353 - val_accuracy: 0.5779\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5569 - accuracy: 0.5384 - val_loss: 0.5292 - val_accuracy: 0.5820\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5506 - accuracy: 0.5494 - val_loss: 0.5364 - val_accuracy: 0.5474\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5521 - accuracy: 0.5505 - val_loss: 0.5159 - val_accuracy: 0.6109\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5517 - accuracy: 0.5428 - val_loss: 0.5266 - val_accuracy: 0.6051\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5546 - accuracy: 0.5458 - val_loss: 0.5252 - val_accuracy: 0.6059\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5500 - accuracy: 0.5571 - val_loss: 0.5274 - val_accuracy: 0.5853\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5506 - accuracy: 0.5532 - val_loss: 0.6220 - val_accuracy: 0.4551\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5452 - accuracy: 0.5617 - val_loss: 0.5225 - val_accuracy: 0.5820\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5469 - accuracy: 0.5524 - val_loss: 0.5258 - val_accuracy: 0.5837\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5471 - accuracy: 0.5565 - val_loss: 0.5225 - val_accuracy: 0.5812\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5431 - accuracy: 0.5631 - val_loss: 0.5152 - val_accuracy: 0.5862\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5454 - accuracy: 0.5590 - val_loss: 0.5236 - val_accuracy: 0.5631\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5387 - accuracy: 0.5656 - val_loss: 0.5661 - val_accuracy: 0.5375\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5460 - accuracy: 0.5560 - val_loss: 0.5096 - val_accuracy: 0.5927\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5416 - accuracy: 0.5711 - val_loss: 0.5087 - val_accuracy: 0.5746\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5403 - accuracy: 0.5595 - val_loss: 0.5314 - val_accuracy: 0.5532\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5381 - accuracy: 0.5681 - val_loss: 0.5171 - val_accuracy: 0.6026\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5343 - accuracy: 0.5705 - val_loss: 0.5191 - val_accuracy: 0.5845\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5422 - accuracy: 0.5604 - val_loss: 0.4991 - val_accuracy: 0.5927\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5384 - accuracy: 0.5670 - val_loss: 0.5160 - val_accuracy: 0.5730\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5396 - accuracy: 0.5593 - val_loss: 0.5112 - val_accuracy: 0.5820\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5372 - accuracy: 0.5675 - val_loss: 0.5148 - val_accuracy: 0.5276\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5357 - accuracy: 0.5681 - val_loss: 0.5128 - val_accuracy: 0.5804\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5311 - accuracy: 0.5700 - val_loss: 0.4997 - val_accuracy: 0.5894\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5332 - accuracy: 0.5738 - val_loss: 0.5038 - val_accuracy: 0.5878\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5296 - accuracy: 0.5763 - val_loss: 0.5011 - val_accuracy: 0.6092\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5281 - accuracy: 0.5716 - val_loss: 0.5136 - val_accuracy: 0.5927\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5242 - accuracy: 0.5878 - val_loss: 0.5361 - val_accuracy: 0.5664\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5288 - accuracy: 0.5678 - val_loss: 0.4910 - val_accuracy: 0.6208\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5276 - accuracy: 0.5763 - val_loss: 0.5073 - val_accuracy: 0.5721\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5275 - accuracy: 0.5768 - val_loss: 0.5056 - val_accuracy: 0.5960\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5266 - accuracy: 0.5876 - val_loss: 0.5068 - val_accuracy: 0.5796\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5303 - accuracy: 0.5752 - val_loss: 0.4864 - val_accuracy: 0.6389\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5262 - accuracy: 0.5799 - val_loss: 0.4977 - val_accuracy: 0.6101\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5219 - accuracy: 0.5966 - val_loss: 0.4947 - val_accuracy: 0.5870\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5227 - accuracy: 0.5829 - val_loss: 0.4912 - val_accuracy: 0.6307\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5205 - accuracy: 0.5917 - val_loss: 0.4916 - val_accuracy: 0.6298\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.5197 - accuracy: 0.5922 - val_loss: 0.4817 - val_accuracy: 0.6389\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5221 - accuracy: 0.5903 - val_loss: 0.5052 - val_accuracy: 0.5820\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5173 - accuracy: 0.5895 - val_loss: 0.4750 - val_accuracy: 0.6463\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5198 - accuracy: 0.5969 - val_loss: 0.4801 - val_accuracy: 0.6620\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5164 - accuracy: 0.5848 - val_loss: 0.5094 - val_accuracy: 0.5927\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5223 - accuracy: 0.5757 - val_loss: 0.4950 - val_accuracy: 0.5688\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5132 - accuracy: 0.5975 - val_loss: 0.4718 - val_accuracy: 0.6381\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5168 - accuracy: 0.5818 - val_loss: 0.4937 - val_accuracy: 0.5738\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5157 - accuracy: 0.5878 - val_loss: 0.4809 - val_accuracy: 0.6356\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5077 - accuracy: 0.5955 - val_loss: 0.7153 - val_accuracy: 0.4749\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5145 - accuracy: 0.5920 - val_loss: 0.5117 - val_accuracy: 0.5919\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5158 - accuracy: 0.5873 - val_loss: 0.4690 - val_accuracy: 0.6496\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5148 - accuracy: 0.5845 - val_loss: 0.5119 - val_accuracy: 0.5680\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5077 - accuracy: 0.5922 - val_loss: 0.5648 - val_accuracy: 0.5499\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5128 - accuracy: 0.5917 - val_loss: 0.4723 - val_accuracy: 0.6265\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5108 - accuracy: 0.6052 - val_loss: 0.4686 - val_accuracy: 0.6529\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5062 - accuracy: 0.5961 - val_loss: 0.4619 - val_accuracy: 0.6430\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5136 - accuracy: 0.5887 - val_loss: 0.4605 - val_accuracy: 0.6406\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5093 - accuracy: 0.6043 - val_loss: 0.4642 - val_accuracy: 0.6422\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5089 - accuracy: 0.5961 - val_loss: 0.4818 - val_accuracy: 0.6274\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5040 - accuracy: 0.6005 - val_loss: 0.5071 - val_accuracy: 0.6167\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5059 - accuracy: 0.5969 - val_loss: 0.4478 - val_accuracy: 0.6628\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5070 - accuracy: 0.6074 - val_loss: 0.4489 - val_accuracy: 0.6472\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5013 - accuracy: 0.6170 - val_loss: 0.4679 - val_accuracy: 0.6274\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5106 - accuracy: 0.5953 - val_loss: 0.4760 - val_accuracy: 0.6702\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5057 - accuracy: 0.6021 - val_loss: 0.4837 - val_accuracy: 0.6232\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5077 - accuracy: 0.5980 - val_loss: 0.4790 - val_accuracy: 0.5969\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5053 - accuracy: 0.5994 - val_loss: 0.5431 - val_accuracy: 0.5672\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4992 - accuracy: 0.6101 - val_loss: 0.4537 - val_accuracy: 0.6488\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5052 - accuracy: 0.5991 - val_loss: 0.5175 - val_accuracy: 0.5911\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5015 - accuracy: 0.6008 - val_loss: 0.4784 - val_accuracy: 0.6529\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5036 - accuracy: 0.6049 - val_loss: 0.5324 - val_accuracy: 0.5936\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5069 - accuracy: 0.6093 - val_loss: 0.4993 - val_accuracy: 0.5532\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5006 - accuracy: 0.6131 - val_loss: 0.4555 - val_accuracy: 0.6389\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5039 - accuracy: 0.6043 - val_loss: 0.4702 - val_accuracy: 0.6224\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5028 - accuracy: 0.6123 - val_loss: 0.4905 - val_accuracy: 0.6538\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4981 - accuracy: 0.5983 - val_loss: 0.4635 - val_accuracy: 0.6430\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4962 - accuracy: 0.6101 - val_loss: 0.4725 - val_accuracy: 0.6249\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5045 - accuracy: 0.6019 - val_loss: 0.4618 - val_accuracy: 0.6727\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4934 - accuracy: 0.6167 - val_loss: 0.4572 - val_accuracy: 0.6843\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4950 - accuracy: 0.6162 - val_loss: 0.4446 - val_accuracy: 0.6735\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4930 - accuracy: 0.6175 - val_loss: 0.4743 - val_accuracy: 0.6521\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4971 - accuracy: 0.6164 - val_loss: 0.4645 - val_accuracy: 0.6364\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5037 - accuracy: 0.6068 - val_loss: 0.4427 - val_accuracy: 0.6818\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4932 - accuracy: 0.6153 - val_loss: 0.5030 - val_accuracy: 0.6711\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4996 - accuracy: 0.6214 - val_loss: 0.4615 - val_accuracy: 0.6430\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4910 - accuracy: 0.6076 - val_loss: 0.4397 - val_accuracy: 0.6645\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4955 - accuracy: 0.6189 - val_loss: 0.4539 - val_accuracy: 0.6579\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4922 - accuracy: 0.6140 - val_loss: 0.4806 - val_accuracy: 0.6480\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4999 - accuracy: 0.6021 - val_loss: 0.4504 - val_accuracy: 0.6711\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4919 - accuracy: 0.6189 - val_loss: 0.4558 - val_accuracy: 0.6455\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4900 - accuracy: 0.6299 - val_loss: 0.4487 - val_accuracy: 0.6430\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4946 - accuracy: 0.6219 - val_loss: 0.4363 - val_accuracy: 0.6826\n",
            "243/243 [==============================] - 1s 3ms/step - loss: 0.4363 - accuracy: 0.6826\n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.050000\n",
            "\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 12s 10ms/step - loss: 0.5945 - accuracy: 0.4784 - val_loss: 0.5780 - val_accuracy: 0.4889\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5779 - accuracy: 0.5073 - val_loss: 0.5591 - val_accuracy: 0.5103\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5702 - accuracy: 0.5186 - val_loss: 0.5563 - val_accuracy: 0.5161\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5662 - accuracy: 0.5271 - val_loss: 0.5504 - val_accuracy: 0.5680\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5672 - accuracy: 0.5282 - val_loss: 0.5465 - val_accuracy: 0.5647\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5626 - accuracy: 0.5304 - val_loss: 0.5632 - val_accuracy: 0.5507\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5625 - accuracy: 0.5351 - val_loss: 0.5421 - val_accuracy: 0.5812\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5627 - accuracy: 0.5326 - val_loss: 0.5499 - val_accuracy: 0.5664\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5598 - accuracy: 0.5342 - val_loss: 0.5448 - val_accuracy: 0.5713\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5579 - accuracy: 0.5342 - val_loss: 0.5441 - val_accuracy: 0.5425\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5582 - accuracy: 0.5304 - val_loss: 0.5427 - val_accuracy: 0.5647\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5522 - accuracy: 0.5447 - val_loss: 0.5280 - val_accuracy: 0.5911\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5542 - accuracy: 0.5370 - val_loss: 0.5272 - val_accuracy: 0.5878\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5555 - accuracy: 0.5452 - val_loss: 0.5345 - val_accuracy: 0.5688\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5533 - accuracy: 0.5469 - val_loss: 0.5425 - val_accuracy: 0.5523\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5514 - accuracy: 0.5524 - val_loss: 0.5256 - val_accuracy: 0.5573\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5504 - accuracy: 0.5494 - val_loss: 0.5657 - val_accuracy: 0.5317\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5431 - accuracy: 0.5667 - val_loss: 0.5144 - val_accuracy: 0.5870\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5482 - accuracy: 0.5483 - val_loss: 0.5115 - val_accuracy: 0.5911\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5459 - accuracy: 0.5554 - val_loss: 0.5312 - val_accuracy: 0.5647\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5438 - accuracy: 0.5532 - val_loss: 0.5558 - val_accuracy: 0.5441\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5476 - accuracy: 0.5496 - val_loss: 0.5402 - val_accuracy: 0.5672\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5418 - accuracy: 0.5590 - val_loss: 0.5561 - val_accuracy: 0.4955\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5446 - accuracy: 0.5579 - val_loss: 0.5157 - val_accuracy: 0.5870\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5402 - accuracy: 0.5584 - val_loss: 0.5126 - val_accuracy: 0.6224\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5389 - accuracy: 0.5565 - val_loss: 0.5218 - val_accuracy: 0.6059\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5409 - accuracy: 0.5631 - val_loss: 0.5359 - val_accuracy: 0.5779\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5362 - accuracy: 0.5766 - val_loss: 0.5404 - val_accuracy: 0.5301\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5404 - accuracy: 0.5689 - val_loss: 0.5107 - val_accuracy: 0.6043\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5335 - accuracy: 0.5777 - val_loss: 0.5321 - val_accuracy: 0.5878\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5352 - accuracy: 0.5672 - val_loss: 0.5469 - val_accuracy: 0.5697\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5338 - accuracy: 0.5650 - val_loss: 0.5216 - val_accuracy: 0.6158\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5366 - accuracy: 0.5708 - val_loss: 0.5390 - val_accuracy: 0.5952\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5333 - accuracy: 0.5752 - val_loss: 0.5136 - val_accuracy: 0.6191\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5330 - accuracy: 0.5700 - val_loss: 0.5081 - val_accuracy: 0.5952\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5363 - accuracy: 0.5716 - val_loss: 0.5146 - val_accuracy: 0.6249\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5286 - accuracy: 0.5771 - val_loss: 0.5057 - val_accuracy: 0.6381\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5269 - accuracy: 0.5749 - val_loss: 0.5297 - val_accuracy: 0.6158\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5253 - accuracy: 0.5812 - val_loss: 0.5020 - val_accuracy: 0.6208\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5321 - accuracy: 0.5837 - val_loss: 0.5043 - val_accuracy: 0.6200\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5251 - accuracy: 0.5777 - val_loss: 0.5043 - val_accuracy: 0.6059\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5306 - accuracy: 0.5799 - val_loss: 0.4945 - val_accuracy: 0.6257\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5270 - accuracy: 0.5829 - val_loss: 0.5612 - val_accuracy: 0.5787\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5255 - accuracy: 0.5796 - val_loss: 0.5187 - val_accuracy: 0.6125\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5248 - accuracy: 0.5826 - val_loss: 0.5273 - val_accuracy: 0.6051\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5225 - accuracy: 0.5854 - val_loss: 0.5145 - val_accuracy: 0.6150\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5259 - accuracy: 0.5716 - val_loss: 0.5303 - val_accuracy: 0.6043\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5189 - accuracy: 0.5953 - val_loss: 0.4904 - val_accuracy: 0.6298\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5158 - accuracy: 0.5922 - val_loss: 0.4986 - val_accuracy: 0.6134\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5220 - accuracy: 0.5854 - val_loss: 0.4856 - val_accuracy: 0.6439\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5166 - accuracy: 0.5986 - val_loss: 0.4962 - val_accuracy: 0.6092\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5184 - accuracy: 0.5859 - val_loss: 0.4969 - val_accuracy: 0.5985\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5195 - accuracy: 0.5859 - val_loss: 0.4881 - val_accuracy: 0.6183\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5076 - accuracy: 0.6063 - val_loss: 0.5048 - val_accuracy: 0.5441\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5153 - accuracy: 0.5900 - val_loss: 0.5289 - val_accuracy: 0.5268\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5191 - accuracy: 0.5818 - val_loss: 0.4886 - val_accuracy: 0.6216\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5059 - accuracy: 0.5988 - val_loss: 0.4973 - val_accuracy: 0.5614\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5164 - accuracy: 0.5851 - val_loss: 0.4959 - val_accuracy: 0.6257\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5099 - accuracy: 0.5964 - val_loss: 0.5246 - val_accuracy: 0.5375\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5205 - accuracy: 0.5832 - val_loss: 0.5004 - val_accuracy: 0.6505\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5134 - accuracy: 0.5851 - val_loss: 0.4923 - val_accuracy: 0.6422\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5045 - accuracy: 0.6030 - val_loss: 0.5426 - val_accuracy: 0.5812\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5141 - accuracy: 0.5884 - val_loss: 0.4753 - val_accuracy: 0.6488\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5075 - accuracy: 0.5944 - val_loss: 0.5208 - val_accuracy: 0.5993\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5110 - accuracy: 0.5887 - val_loss: 0.4932 - val_accuracy: 0.6216\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5113 - accuracy: 0.5966 - val_loss: 0.4942 - val_accuracy: 0.6298\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5091 - accuracy: 0.5936 - val_loss: 0.4945 - val_accuracy: 0.6117\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5022 - accuracy: 0.6021 - val_loss: 0.4965 - val_accuracy: 0.6315\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5011 - accuracy: 0.5994 - val_loss: 0.5191 - val_accuracy: 0.6373\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5030 - accuracy: 0.5953 - val_loss: 0.4871 - val_accuracy: 0.6373\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5051 - accuracy: 0.5966 - val_loss: 0.4882 - val_accuracy: 0.6496\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5041 - accuracy: 0.6052 - val_loss: 0.4683 - val_accuracy: 0.6430\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5065 - accuracy: 0.6035 - val_loss: 0.5297 - val_accuracy: 0.6092\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.5057 - accuracy: 0.5928 - val_loss: 0.5088 - val_accuracy: 0.5606\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5021 - accuracy: 0.5931 - val_loss: 0.5067 - val_accuracy: 0.6249\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4988 - accuracy: 0.6024 - val_loss: 0.4864 - val_accuracy: 0.6496\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4994 - accuracy: 0.6060 - val_loss: 0.5811 - val_accuracy: 0.6125\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5060 - accuracy: 0.5928 - val_loss: 0.4766 - val_accuracy: 0.6505\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4965 - accuracy: 0.6156 - val_loss: 0.4748 - val_accuracy: 0.6669\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5031 - accuracy: 0.5969 - val_loss: 0.5072 - val_accuracy: 0.6026\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4945 - accuracy: 0.6107 - val_loss: 0.4604 - val_accuracy: 0.6513\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4944 - accuracy: 0.6027 - val_loss: 0.4938 - val_accuracy: 0.6397\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4970 - accuracy: 0.6049 - val_loss: 0.4890 - val_accuracy: 0.6158\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5007 - accuracy: 0.6010 - val_loss: 0.4784 - val_accuracy: 0.6538\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.4980 - accuracy: 0.5983 - val_loss: 0.4695 - val_accuracy: 0.6645\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5017 - accuracy: 0.6021 - val_loss: 0.4907 - val_accuracy: 0.6678\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5002 - accuracy: 0.6074 - val_loss: 0.4535 - val_accuracy: 0.6521\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4974 - accuracy: 0.6060 - val_loss: 0.4598 - val_accuracy: 0.6373\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4922 - accuracy: 0.6054 - val_loss: 0.4658 - val_accuracy: 0.6538\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4980 - accuracy: 0.6052 - val_loss: 0.5393 - val_accuracy: 0.6389\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5020 - accuracy: 0.6120 - val_loss: 0.4765 - val_accuracy: 0.6257\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4910 - accuracy: 0.6153 - val_loss: 0.4536 - val_accuracy: 0.6505\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4950 - accuracy: 0.6054 - val_loss: 0.4652 - val_accuracy: 0.6430\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4877 - accuracy: 0.6101 - val_loss: 0.4976 - val_accuracy: 0.6546\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4980 - accuracy: 0.6076 - val_loss: 0.4790 - val_accuracy: 0.6389\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4877 - accuracy: 0.6203 - val_loss: 0.4721 - val_accuracy: 0.6282\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4979 - accuracy: 0.6057 - val_loss: 0.5139 - val_accuracy: 0.5985\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4870 - accuracy: 0.6189 - val_loss: 0.5391 - val_accuracy: 0.5754\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4926 - accuracy: 0.6162 - val_loss: 0.4782 - val_accuracy: 0.6356\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4880 - accuracy: 0.6162 - val_loss: 0.4637 - val_accuracy: 0.6760\n",
            "243/243 [==============================] - 1s 3ms/step - loss: 0.4637 - accuracy: 0.6760\n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.060000\n",
            "\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 12s 10ms/step - loss: 0.5913 - accuracy: 0.4913 - val_loss: 0.5826 - val_accuracy: 0.4790\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5753 - accuracy: 0.5056 - val_loss: 0.5570 - val_accuracy: 0.5556\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5696 - accuracy: 0.5183 - val_loss: 0.5545 - val_accuracy: 0.5746\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5658 - accuracy: 0.5348 - val_loss: 0.5547 - val_accuracy: 0.5631\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5658 - accuracy: 0.5238 - val_loss: 0.5520 - val_accuracy: 0.5350\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5627 - accuracy: 0.5301 - val_loss: 0.5455 - val_accuracy: 0.5688\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5557 - accuracy: 0.5491 - val_loss: 0.5335 - val_accuracy: 0.5573\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5621 - accuracy: 0.5373 - val_loss: 0.5360 - val_accuracy: 0.5804\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5561 - accuracy: 0.5463 - val_loss: 0.5368 - val_accuracy: 0.5631\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5572 - accuracy: 0.5466 - val_loss: 0.5286 - val_accuracy: 0.5771\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5589 - accuracy: 0.5381 - val_loss: 0.5322 - val_accuracy: 0.5721\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5486 - accuracy: 0.5507 - val_loss: 0.5295 - val_accuracy: 0.5730\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5536 - accuracy: 0.5466 - val_loss: 0.5374 - val_accuracy: 0.5606\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5505 - accuracy: 0.5510 - val_loss: 0.5333 - val_accuracy: 0.5565\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5530 - accuracy: 0.5430 - val_loss: 0.5347 - val_accuracy: 0.5894\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5499 - accuracy: 0.5546 - val_loss: 0.5232 - val_accuracy: 0.5936\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5499 - accuracy: 0.5529 - val_loss: 0.5388 - val_accuracy: 0.5383\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5433 - accuracy: 0.5648 - val_loss: 0.5134 - val_accuracy: 0.5993\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5426 - accuracy: 0.5587 - val_loss: 0.5157 - val_accuracy: 0.5944\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5483 - accuracy: 0.5543 - val_loss: 0.5218 - val_accuracy: 0.5721\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5425 - accuracy: 0.5568 - val_loss: 0.5160 - val_accuracy: 0.5870\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5461 - accuracy: 0.5562 - val_loss: 0.5186 - val_accuracy: 0.5383\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5400 - accuracy: 0.5642 - val_loss: 0.5270 - val_accuracy: 0.5449\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5459 - accuracy: 0.5543 - val_loss: 0.5221 - val_accuracy: 0.5565\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5412 - accuracy: 0.5672 - val_loss: 0.5114 - val_accuracy: 0.5911\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5409 - accuracy: 0.5557 - val_loss: 0.5186 - val_accuracy: 0.5903\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5397 - accuracy: 0.5557 - val_loss: 0.5305 - val_accuracy: 0.5260\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5352 - accuracy: 0.5705 - val_loss: 0.5439 - val_accuracy: 0.5392\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5418 - accuracy: 0.5565 - val_loss: 0.5099 - val_accuracy: 0.5911\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5336 - accuracy: 0.5722 - val_loss: 0.5108 - val_accuracy: 0.5960\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5347 - accuracy: 0.5683 - val_loss: 0.5015 - val_accuracy: 0.5730\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5342 - accuracy: 0.5653 - val_loss: 0.5132 - val_accuracy: 0.5804\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5312 - accuracy: 0.5694 - val_loss: 0.5362 - val_accuracy: 0.5730\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5352 - accuracy: 0.5664 - val_loss: 0.4951 - val_accuracy: 0.6092\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5335 - accuracy: 0.5683 - val_loss: 0.5258 - val_accuracy: 0.5532\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5355 - accuracy: 0.5738 - val_loss: 0.5062 - val_accuracy: 0.5977\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5271 - accuracy: 0.5719 - val_loss: 0.5013 - val_accuracy: 0.5969\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5237 - accuracy: 0.5766 - val_loss: 0.5000 - val_accuracy: 0.5812\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5244 - accuracy: 0.5692 - val_loss: 0.4959 - val_accuracy: 0.6125\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5237 - accuracy: 0.5865 - val_loss: 0.5218 - val_accuracy: 0.5548\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5236 - accuracy: 0.5730 - val_loss: 0.5360 - val_accuracy: 0.5540\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5283 - accuracy: 0.5749 - val_loss: 0.5007 - val_accuracy: 0.5952\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5273 - accuracy: 0.5653 - val_loss: 0.5281 - val_accuracy: 0.5862\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5228 - accuracy: 0.5812 - val_loss: 0.5008 - val_accuracy: 0.5944\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5208 - accuracy: 0.5911 - val_loss: 0.5264 - val_accuracy: 0.5631\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5199 - accuracy: 0.5823 - val_loss: 0.5096 - val_accuracy: 0.6076\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5214 - accuracy: 0.5823 - val_loss: 0.5071 - val_accuracy: 0.5969\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5153 - accuracy: 0.5889 - val_loss: 0.4897 - val_accuracy: 0.6158\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5170 - accuracy: 0.5812 - val_loss: 0.5130 - val_accuracy: 0.5771\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5137 - accuracy: 0.5953 - val_loss: 0.5269 - val_accuracy: 0.5944\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5169 - accuracy: 0.5892 - val_loss: 0.4823 - val_accuracy: 0.6381\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5219 - accuracy: 0.5834 - val_loss: 0.5116 - val_accuracy: 0.5837\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5179 - accuracy: 0.5821 - val_loss: 0.5013 - val_accuracy: 0.6315\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5061 - accuracy: 0.6041 - val_loss: 0.5011 - val_accuracy: 0.5927\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5137 - accuracy: 0.5964 - val_loss: 0.4838 - val_accuracy: 0.6224\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5139 - accuracy: 0.5895 - val_loss: 0.5139 - val_accuracy: 0.6018\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5103 - accuracy: 0.5977 - val_loss: 0.5084 - val_accuracy: 0.6134\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5169 - accuracy: 0.5928 - val_loss: 0.5147 - val_accuracy: 0.6298\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5070 - accuracy: 0.5928 - val_loss: 0.5093 - val_accuracy: 0.6059\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5148 - accuracy: 0.5972 - val_loss: 0.5287 - val_accuracy: 0.6183\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5153 - accuracy: 0.5895 - val_loss: 0.4899 - val_accuracy: 0.6381\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5049 - accuracy: 0.5958 - val_loss: 0.7294 - val_accuracy: 0.5194\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5112 - accuracy: 0.6054 - val_loss: 0.4953 - val_accuracy: 0.6340\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5105 - accuracy: 0.5931 - val_loss: 0.4750 - val_accuracy: 0.6455\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5077 - accuracy: 0.5900 - val_loss: 0.5136 - val_accuracy: 0.6356\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5064 - accuracy: 0.5964 - val_loss: 0.5016 - val_accuracy: 0.6125\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.5023 - accuracy: 0.5942 - val_loss: 0.4717 - val_accuracy: 0.6282\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5019 - accuracy: 0.6071 - val_loss: 0.5175 - val_accuracy: 0.5977\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5047 - accuracy: 0.5936 - val_loss: 0.5845 - val_accuracy: 0.6364\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4962 - accuracy: 0.6063 - val_loss: 0.4712 - val_accuracy: 0.6538\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5018 - accuracy: 0.6054 - val_loss: 0.4675 - val_accuracy: 0.6472\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5005 - accuracy: 0.5980 - val_loss: 0.5848 - val_accuracy: 0.6092\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4963 - accuracy: 0.6170 - val_loss: 0.5117 - val_accuracy: 0.6084\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5034 - accuracy: 0.5977 - val_loss: 0.4672 - val_accuracy: 0.6414\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4963 - accuracy: 0.6068 - val_loss: 0.5126 - val_accuracy: 0.6389\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4895 - accuracy: 0.6225 - val_loss: 0.5229 - val_accuracy: 0.6035\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5002 - accuracy: 0.6016 - val_loss: 0.5370 - val_accuracy: 0.5416\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4898 - accuracy: 0.6192 - val_loss: 0.5323 - val_accuracy: 0.5894\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4921 - accuracy: 0.6217 - val_loss: 0.4579 - val_accuracy: 0.6505\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4966 - accuracy: 0.6087 - val_loss: 0.4863 - val_accuracy: 0.6125\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4890 - accuracy: 0.6076 - val_loss: 0.4544 - val_accuracy: 0.6315\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4904 - accuracy: 0.6211 - val_loss: 0.5028 - val_accuracy: 0.5894\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4986 - accuracy: 0.6060 - val_loss: 0.6091 - val_accuracy: 0.5886\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4920 - accuracy: 0.6170 - val_loss: 0.4765 - val_accuracy: 0.6200\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4886 - accuracy: 0.6208 - val_loss: 0.5651 - val_accuracy: 0.6092\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4944 - accuracy: 0.6118 - val_loss: 0.4855 - val_accuracy: 0.6406\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4930 - accuracy: 0.6101 - val_loss: 0.4665 - val_accuracy: 0.6298\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4905 - accuracy: 0.6101 - val_loss: 0.4612 - val_accuracy: 0.6241\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4826 - accuracy: 0.6184 - val_loss: 0.4947 - val_accuracy: 0.5862\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4839 - accuracy: 0.6299 - val_loss: 0.4805 - val_accuracy: 0.6158\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4909 - accuracy: 0.6145 - val_loss: 0.4993 - val_accuracy: 0.5820\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.4860 - accuracy: 0.6189 - val_loss: 0.4600 - val_accuracy: 0.6241\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4875 - accuracy: 0.6184 - val_loss: 0.5032 - val_accuracy: 0.5713\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4839 - accuracy: 0.6164 - val_loss: 0.4647 - val_accuracy: 0.6397\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4884 - accuracy: 0.6156 - val_loss: 0.4452 - val_accuracy: 0.6529\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4873 - accuracy: 0.6247 - val_loss: 0.4510 - val_accuracy: 0.6183\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4891 - accuracy: 0.6071 - val_loss: 0.4452 - val_accuracy: 0.6620\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4828 - accuracy: 0.6255 - val_loss: 0.4681 - val_accuracy: 0.5977\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4842 - accuracy: 0.6217 - val_loss: 0.4493 - val_accuracy: 0.6661\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4829 - accuracy: 0.6123 - val_loss: 0.4746 - val_accuracy: 0.6669\n",
            "243/243 [==============================] - 1s 4ms/step - loss: 0.4746 - accuracy: 0.6669\n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.070000\n",
            "\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 12s 10ms/step - loss: 0.5943 - accuracy: 0.4834 - val_loss: 0.5692 - val_accuracy: 0.5425\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5765 - accuracy: 0.5087 - val_loss: 0.5643 - val_accuracy: 0.5301\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5719 - accuracy: 0.5241 - val_loss: 0.5775 - val_accuracy: 0.4757\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5654 - accuracy: 0.5337 - val_loss: 0.5650 - val_accuracy: 0.5144\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5714 - accuracy: 0.5227 - val_loss: 0.5526 - val_accuracy: 0.5540\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5671 - accuracy: 0.5257 - val_loss: 0.5498 - val_accuracy: 0.5713\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5660 - accuracy: 0.5400 - val_loss: 0.5777 - val_accuracy: 0.4691\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5674 - accuracy: 0.5243 - val_loss: 0.5505 - val_accuracy: 0.5779\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5642 - accuracy: 0.5417 - val_loss: 0.5484 - val_accuracy: 0.5730\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5621 - accuracy: 0.5411 - val_loss: 0.5505 - val_accuracy: 0.5425\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5608 - accuracy: 0.5444 - val_loss: 0.5364 - val_accuracy: 0.5697\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5549 - accuracy: 0.5551 - val_loss: 0.5358 - val_accuracy: 0.5870\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5574 - accuracy: 0.5447 - val_loss: 0.5522 - val_accuracy: 0.5400\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5573 - accuracy: 0.5458 - val_loss: 0.5508 - val_accuracy: 0.5647\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5569 - accuracy: 0.5499 - val_loss: 0.5633 - val_accuracy: 0.4880\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5566 - accuracy: 0.5496 - val_loss: 0.5325 - val_accuracy: 0.5812\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5544 - accuracy: 0.5518 - val_loss: 0.6093 - val_accuracy: 0.4782\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5538 - accuracy: 0.5584 - val_loss: 0.5326 - val_accuracy: 0.5730\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5520 - accuracy: 0.5494 - val_loss: 0.5257 - val_accuracy: 0.5655\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5515 - accuracy: 0.5582 - val_loss: 0.5460 - val_accuracy: 0.5581\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5485 - accuracy: 0.5540 - val_loss: 0.5235 - val_accuracy: 0.5779\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5503 - accuracy: 0.5560 - val_loss: 0.5167 - val_accuracy: 0.6200\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5475 - accuracy: 0.5615 - val_loss: 0.5230 - val_accuracy: 0.5960\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5498 - accuracy: 0.5637 - val_loss: 0.5191 - val_accuracy: 0.6002\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5495 - accuracy: 0.5527 - val_loss: 0.5272 - val_accuracy: 0.5647\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5477 - accuracy: 0.5538 - val_loss: 0.5128 - val_accuracy: 0.6183\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5453 - accuracy: 0.5623 - val_loss: 0.5307 - val_accuracy: 0.5639\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5392 - accuracy: 0.5716 - val_loss: 0.5089 - val_accuracy: 0.5977\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5465 - accuracy: 0.5507 - val_loss: 0.5129 - val_accuracy: 0.5862\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5413 - accuracy: 0.5648 - val_loss: 0.5120 - val_accuracy: 0.5960\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5469 - accuracy: 0.5637 - val_loss: 0.5165 - val_accuracy: 0.5960\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5402 - accuracy: 0.5645 - val_loss: 0.5124 - val_accuracy: 0.6043\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5417 - accuracy: 0.5612 - val_loss: 0.5212 - val_accuracy: 0.5993\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5400 - accuracy: 0.5661 - val_loss: 0.5191 - val_accuracy: 0.5787\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5376 - accuracy: 0.5631 - val_loss: 0.5184 - val_accuracy: 0.5738\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5437 - accuracy: 0.5593 - val_loss: 0.5113 - val_accuracy: 0.6307\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5382 - accuracy: 0.5730 - val_loss: 0.5102 - val_accuracy: 0.6183\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5372 - accuracy: 0.5708 - val_loss: 0.5136 - val_accuracy: 0.6331\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5359 - accuracy: 0.5700 - val_loss: 0.5151 - val_accuracy: 0.5771\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5374 - accuracy: 0.5768 - val_loss: 0.5056 - val_accuracy: 0.6158\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5320 - accuracy: 0.5774 - val_loss: 0.5024 - val_accuracy: 0.6430\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5347 - accuracy: 0.5777 - val_loss: 0.5405 - val_accuracy: 0.5449\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5388 - accuracy: 0.5774 - val_loss: 0.5217 - val_accuracy: 0.6092\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5296 - accuracy: 0.5834 - val_loss: 0.5304 - val_accuracy: 0.5210\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5308 - accuracy: 0.5906 - val_loss: 0.4962 - val_accuracy: 0.6315\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5317 - accuracy: 0.5840 - val_loss: 0.5086 - val_accuracy: 0.6315\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5327 - accuracy: 0.5837 - val_loss: 0.4942 - val_accuracy: 0.6397\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5299 - accuracy: 0.5708 - val_loss: 0.5154 - val_accuracy: 0.5886\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5309 - accuracy: 0.5865 - val_loss: 0.5172 - val_accuracy: 0.5507\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5319 - accuracy: 0.5763 - val_loss: 0.5525 - val_accuracy: 0.5672\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5290 - accuracy: 0.5766 - val_loss: 0.4985 - val_accuracy: 0.6249\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5270 - accuracy: 0.5920 - val_loss: 0.5490 - val_accuracy: 0.5647\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5302 - accuracy: 0.5843 - val_loss: 0.5163 - val_accuracy: 0.5853\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5210 - accuracy: 0.5942 - val_loss: 0.5006 - val_accuracy: 0.5936\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5268 - accuracy: 0.5782 - val_loss: 0.5094 - val_accuracy: 0.6109\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5243 - accuracy: 0.5760 - val_loss: 0.4878 - val_accuracy: 0.6480\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5202 - accuracy: 0.5876 - val_loss: 0.5049 - val_accuracy: 0.6422\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5205 - accuracy: 0.5856 - val_loss: 0.5171 - val_accuracy: 0.5894\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5216 - accuracy: 0.5928 - val_loss: 0.5037 - val_accuracy: 0.6439\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 10s 14ms/step - loss: 0.5285 - accuracy: 0.5818 - val_loss: 0.5022 - val_accuracy: 0.6142\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5232 - accuracy: 0.5832 - val_loss: 0.4938 - val_accuracy: 0.6249\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5196 - accuracy: 0.5862 - val_loss: 0.4914 - val_accuracy: 0.6208\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5232 - accuracy: 0.5862 - val_loss: 0.4862 - val_accuracy: 0.6463\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5206 - accuracy: 0.5812 - val_loss: 0.4738 - val_accuracy: 0.6307\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5210 - accuracy: 0.5906 - val_loss: 0.4842 - val_accuracy: 0.6364\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5217 - accuracy: 0.5757 - val_loss: 0.4847 - val_accuracy: 0.5911\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5158 - accuracy: 0.5966 - val_loss: 0.4824 - val_accuracy: 0.6208\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5154 - accuracy: 0.5865 - val_loss: 0.4797 - val_accuracy: 0.6315\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5182 - accuracy: 0.5917 - val_loss: 0.4770 - val_accuracy: 0.6463\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5167 - accuracy: 0.5961 - val_loss: 0.5085 - val_accuracy: 0.5936\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5205 - accuracy: 0.5933 - val_loss: 0.4919 - val_accuracy: 0.6109\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5176 - accuracy: 0.5873 - val_loss: 0.5023 - val_accuracy: 0.6035\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5115 - accuracy: 0.5999 - val_loss: 0.4648 - val_accuracy: 0.6678\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5158 - accuracy: 0.5859 - val_loss: 0.4961 - val_accuracy: 0.5639\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5112 - accuracy: 0.6019 - val_loss: 0.4816 - val_accuracy: 0.6414\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5163 - accuracy: 0.5950 - val_loss: 0.4755 - val_accuracy: 0.6414\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5150 - accuracy: 0.5859 - val_loss: 0.4863 - val_accuracy: 0.6298\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5123 - accuracy: 0.6068 - val_loss: 0.5028 - val_accuracy: 0.6043\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5060 - accuracy: 0.6082 - val_loss: 0.4774 - val_accuracy: 0.6290\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5132 - accuracy: 0.5911 - val_loss: 0.4762 - val_accuracy: 0.6208\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5077 - accuracy: 0.6068 - val_loss: 0.4562 - val_accuracy: 0.6628\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5067 - accuracy: 0.5977 - val_loss: 0.4951 - val_accuracy: 0.6257\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5072 - accuracy: 0.5991 - val_loss: 0.4956 - val_accuracy: 0.6002\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5088 - accuracy: 0.6030 - val_loss: 0.4622 - val_accuracy: 0.6694\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5083 - accuracy: 0.6071 - val_loss: 0.4680 - val_accuracy: 0.6521\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5139 - accuracy: 0.5939 - val_loss: 0.4795 - val_accuracy: 0.6430\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5054 - accuracy: 0.6115 - val_loss: 0.4741 - val_accuracy: 0.6513\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5011 - accuracy: 0.6065 - val_loss: 0.5316 - val_accuracy: 0.6125\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5031 - accuracy: 0.6074 - val_loss: 0.4588 - val_accuracy: 0.6669\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5050 - accuracy: 0.6082 - val_loss: 0.4780 - val_accuracy: 0.6496\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5102 - accuracy: 0.5961 - val_loss: 0.4586 - val_accuracy: 0.6439\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5042 - accuracy: 0.6065 - val_loss: 0.5004 - val_accuracy: 0.6183\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5050 - accuracy: 0.5980 - val_loss: 0.5135 - val_accuracy: 0.5771\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5022 - accuracy: 0.6032 - val_loss: 0.4649 - val_accuracy: 0.6447\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5022 - accuracy: 0.6063 - val_loss: 0.4903 - val_accuracy: 0.6406\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5022 - accuracy: 0.5991 - val_loss: 0.4958 - val_accuracy: 0.5754\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 8s 11ms/step - loss: 0.5008 - accuracy: 0.5953 - val_loss: 0.4557 - val_accuracy: 0.6777\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5036 - accuracy: 0.6076 - val_loss: 0.5037 - val_accuracy: 0.5993\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4945 - accuracy: 0.6096 - val_loss: 0.4811 - val_accuracy: 0.6290\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5027 - accuracy: 0.6049 - val_loss: 0.4751 - val_accuracy: 0.6595\n",
            "243/243 [==============================] - 1s 4ms/step - loss: 0.4751 - accuracy: 0.6595\n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.080000\n",
            "\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 11s 9ms/step - loss: 0.5880 - accuracy: 0.4828 - val_loss: 0.5799 - val_accuracy: 0.5087\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5785 - accuracy: 0.4957 - val_loss: 0.5742 - val_accuracy: 0.5499\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5728 - accuracy: 0.5092 - val_loss: 0.5652 - val_accuracy: 0.5688\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5676 - accuracy: 0.5279 - val_loss: 0.5611 - val_accuracy: 0.5556\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5717 - accuracy: 0.5263 - val_loss: 0.5510 - val_accuracy: 0.5738\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5680 - accuracy: 0.5199 - val_loss: 0.5519 - val_accuracy: 0.5606\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5645 - accuracy: 0.5400 - val_loss: 0.5453 - val_accuracy: 0.5787\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5648 - accuracy: 0.5285 - val_loss: 0.5554 - val_accuracy: 0.5581\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5633 - accuracy: 0.5455 - val_loss: 0.5440 - val_accuracy: 0.5697\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5622 - accuracy: 0.5397 - val_loss: 0.5550 - val_accuracy: 0.5301\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5599 - accuracy: 0.5430 - val_loss: 0.5427 - val_accuracy: 0.5812\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5548 - accuracy: 0.5474 - val_loss: 0.5373 - val_accuracy: 0.6051\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5575 - accuracy: 0.5458 - val_loss: 0.5963 - val_accuracy: 0.5523\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5578 - accuracy: 0.5408 - val_loss: 0.5317 - val_accuracy: 0.5779\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5601 - accuracy: 0.5392 - val_loss: 0.5417 - val_accuracy: 0.5556\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5577 - accuracy: 0.5483 - val_loss: 0.5640 - val_accuracy: 0.5301\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5561 - accuracy: 0.5521 - val_loss: 0.5442 - val_accuracy: 0.5540\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5524 - accuracy: 0.5474 - val_loss: 0.5478 - val_accuracy: 0.5631\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5508 - accuracy: 0.5496 - val_loss: 0.5272 - val_accuracy: 0.5845\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5527 - accuracy: 0.5642 - val_loss: 0.5273 - val_accuracy: 0.5837\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5489 - accuracy: 0.5532 - val_loss: 0.5164 - val_accuracy: 0.6010\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5534 - accuracy: 0.5505 - val_loss: 0.5353 - val_accuracy: 0.5862\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5441 - accuracy: 0.5634 - val_loss: 0.5272 - val_accuracy: 0.5763\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5510 - accuracy: 0.5527 - val_loss: 0.5189 - val_accuracy: 0.6018\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5478 - accuracy: 0.5527 - val_loss: 0.5173 - val_accuracy: 0.6084\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5472 - accuracy: 0.5488 - val_loss: 0.5160 - val_accuracy: 0.5787\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5487 - accuracy: 0.5485 - val_loss: 0.5123 - val_accuracy: 0.6010\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5444 - accuracy: 0.5637 - val_loss: 0.5078 - val_accuracy: 0.6002\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5473 - accuracy: 0.5595 - val_loss: 0.5059 - val_accuracy: 0.6191\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5430 - accuracy: 0.5628 - val_loss: 0.4958 - val_accuracy: 0.6232\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5476 - accuracy: 0.5546 - val_loss: 0.5104 - val_accuracy: 0.5746\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5399 - accuracy: 0.5645 - val_loss: 0.5197 - val_accuracy: 0.5573\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5461 - accuracy: 0.5565 - val_loss: 0.5174 - val_accuracy: 0.5664\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5429 - accuracy: 0.5571 - val_loss: 0.5065 - val_accuracy: 0.6051\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5404 - accuracy: 0.5692 - val_loss: 0.5134 - val_accuracy: 0.5796\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5459 - accuracy: 0.5527 - val_loss: 0.5253 - val_accuracy: 0.5919\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5382 - accuracy: 0.5741 - val_loss: 0.5256 - val_accuracy: 0.5705\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5366 - accuracy: 0.5738 - val_loss: 0.5202 - val_accuracy: 0.5746\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5398 - accuracy: 0.5675 - val_loss: 0.5118 - val_accuracy: 0.5787\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5399 - accuracy: 0.5637 - val_loss: 0.5450 - val_accuracy: 0.5202\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5376 - accuracy: 0.5617 - val_loss: 0.4914 - val_accuracy: 0.6183\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5363 - accuracy: 0.5768 - val_loss: 0.5029 - val_accuracy: 0.6109\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5397 - accuracy: 0.5667 - val_loss: 0.4996 - val_accuracy: 0.6051\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5350 - accuracy: 0.5631 - val_loss: 0.4904 - val_accuracy: 0.6150\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5306 - accuracy: 0.5826 - val_loss: 0.5429 - val_accuracy: 0.5350\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5313 - accuracy: 0.5755 - val_loss: 0.4897 - val_accuracy: 0.6191\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5360 - accuracy: 0.5777 - val_loss: 0.4965 - val_accuracy: 0.5993\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5297 - accuracy: 0.5766 - val_loss: 0.5093 - val_accuracy: 0.5705\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5286 - accuracy: 0.5821 - val_loss: 0.5553 - val_accuracy: 0.5128\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5250 - accuracy: 0.5884 - val_loss: 0.4964 - val_accuracy: 0.6241\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5296 - accuracy: 0.5832 - val_loss: 0.4968 - val_accuracy: 0.5919\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5291 - accuracy: 0.5700 - val_loss: 0.6000 - val_accuracy: 0.5969\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5257 - accuracy: 0.5727 - val_loss: 0.5185 - val_accuracy: 0.5589\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5284 - accuracy: 0.5768 - val_loss: 0.4914 - val_accuracy: 0.5862\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5307 - accuracy: 0.5884 - val_loss: 0.4884 - val_accuracy: 0.6043\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5260 - accuracy: 0.5746 - val_loss: 0.4862 - val_accuracy: 0.6414\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5272 - accuracy: 0.5812 - val_loss: 0.4937 - val_accuracy: 0.6274\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5260 - accuracy: 0.5785 - val_loss: 0.4966 - val_accuracy: 0.6035\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5259 - accuracy: 0.5812 - val_loss: 0.5149 - val_accuracy: 0.5688\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5258 - accuracy: 0.5735 - val_loss: 0.4918 - val_accuracy: 0.5837\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5221 - accuracy: 0.5724 - val_loss: 0.4915 - val_accuracy: 0.5977\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5189 - accuracy: 0.5887 - val_loss: 0.6176 - val_accuracy: 0.5078\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5218 - accuracy: 0.5950 - val_loss: 0.4769 - val_accuracy: 0.6364\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5164 - accuracy: 0.5870 - val_loss: 0.4844 - val_accuracy: 0.6323\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5199 - accuracy: 0.5887 - val_loss: 0.5739 - val_accuracy: 0.5317\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5224 - accuracy: 0.5818 - val_loss: 0.4815 - val_accuracy: 0.6290\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5221 - accuracy: 0.5738 - val_loss: 0.4715 - val_accuracy: 0.6406\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5126 - accuracy: 0.5917 - val_loss: 0.5017 - val_accuracy: 0.5960\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5163 - accuracy: 0.5862 - val_loss: 0.4710 - val_accuracy: 0.6455\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5140 - accuracy: 0.5953 - val_loss: 0.4765 - val_accuracy: 0.6183\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5183 - accuracy: 0.5889 - val_loss: 0.4866 - val_accuracy: 0.5969\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5171 - accuracy: 0.5856 - val_loss: 0.4904 - val_accuracy: 0.6035\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5161 - accuracy: 0.5895 - val_loss: 0.4929 - val_accuracy: 0.6018\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5158 - accuracy: 0.5843 - val_loss: 0.4812 - val_accuracy: 0.6175\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5154 - accuracy: 0.6010 - val_loss: 0.4749 - val_accuracy: 0.6414\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5105 - accuracy: 0.6008 - val_loss: 0.4631 - val_accuracy: 0.6307\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5148 - accuracy: 0.5953 - val_loss: 0.4839 - val_accuracy: 0.6158\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5099 - accuracy: 0.5906 - val_loss: 0.4879 - val_accuracy: 0.6167\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5056 - accuracy: 0.6021 - val_loss: 0.4714 - val_accuracy: 0.6496\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5106 - accuracy: 0.5933 - val_loss: 0.4816 - val_accuracy: 0.6043\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5084 - accuracy: 0.6010 - val_loss: 0.4713 - val_accuracy: 0.6373\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5056 - accuracy: 0.5958 - val_loss: 0.4769 - val_accuracy: 0.6340\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5093 - accuracy: 0.5873 - val_loss: 0.4639 - val_accuracy: 0.6315\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4991 - accuracy: 0.6030 - val_loss: 0.4912 - val_accuracy: 0.6051\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5102 - accuracy: 0.5983 - val_loss: 0.4793 - val_accuracy: 0.6356\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5087 - accuracy: 0.5980 - val_loss: 0.4565 - val_accuracy: 0.6595\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5024 - accuracy: 0.6082 - val_loss: 0.4770 - val_accuracy: 0.6274\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5010 - accuracy: 0.6049 - val_loss: 0.4707 - val_accuracy: 0.6117\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5001 - accuracy: 0.6123 - val_loss: 0.4903 - val_accuracy: 0.6200\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5050 - accuracy: 0.6090 - val_loss: 0.4500 - val_accuracy: 0.6686\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5071 - accuracy: 0.6027 - val_loss: 0.4777 - val_accuracy: 0.6488\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4983 - accuracy: 0.6087 - val_loss: 0.4905 - val_accuracy: 0.6035\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5049 - accuracy: 0.6043 - val_loss: 0.4879 - val_accuracy: 0.6084\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4997 - accuracy: 0.6054 - val_loss: 0.4396 - val_accuracy: 0.6752\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5024 - accuracy: 0.6093 - val_loss: 0.4617 - val_accuracy: 0.6529\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5025 - accuracy: 0.6060 - val_loss: 0.4530 - val_accuracy: 0.6843\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5032 - accuracy: 0.5986 - val_loss: 0.4586 - val_accuracy: 0.6826\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4999 - accuracy: 0.6093 - val_loss: 0.4783 - val_accuracy: 0.6579\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4988 - accuracy: 0.6082 - val_loss: 0.4718 - val_accuracy: 0.6381\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4999 - accuracy: 0.6093 - val_loss: 0.4655 - val_accuracy: 0.6406\n",
            "243/243 [==============================] - 1s 4ms/step - loss: 0.4655 - accuracy: 0.6406\n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.090000\n",
            "\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 13s 11ms/step - loss: 0.5978 - accuracy: 0.4669 - val_loss: 0.5763 - val_accuracy: 0.4831\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5791 - accuracy: 0.4990 - val_loss: 0.5676 - val_accuracy: 0.5268\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5731 - accuracy: 0.5216 - val_loss: 0.5616 - val_accuracy: 0.5523\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5689 - accuracy: 0.5257 - val_loss: 0.5556 - val_accuracy: 0.5317\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5728 - accuracy: 0.5164 - val_loss: 0.5522 - val_accuracy: 0.5664\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5651 - accuracy: 0.5282 - val_loss: 0.5720 - val_accuracy: 0.4930\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5632 - accuracy: 0.5318 - val_loss: 0.5551 - val_accuracy: 0.4963\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5670 - accuracy: 0.5276 - val_loss: 0.5520 - val_accuracy: 0.5449\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5621 - accuracy: 0.5397 - val_loss: 0.5460 - val_accuracy: 0.5540\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5621 - accuracy: 0.5296 - val_loss: 0.5413 - val_accuracy: 0.5639\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5616 - accuracy: 0.5463 - val_loss: 0.5441 - val_accuracy: 0.5474\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5573 - accuracy: 0.5400 - val_loss: 0.5283 - val_accuracy: 0.5853\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5583 - accuracy: 0.5331 - val_loss: 0.5462 - val_accuracy: 0.5507\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5548 - accuracy: 0.5433 - val_loss: 0.5312 - val_accuracy: 0.5482\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5605 - accuracy: 0.5326 - val_loss: 0.5583 - val_accuracy: 0.5383\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5585 - accuracy: 0.5384 - val_loss: 0.5341 - val_accuracy: 0.5227\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5540 - accuracy: 0.5532 - val_loss: 0.5793 - val_accuracy: 0.4922\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5544 - accuracy: 0.5430 - val_loss: 0.5393 - val_accuracy: 0.5441\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5508 - accuracy: 0.5450 - val_loss: 0.5340 - val_accuracy: 0.5416\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5513 - accuracy: 0.5576 - val_loss: 0.5218 - val_accuracy: 0.5796\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5476 - accuracy: 0.5450 - val_loss: 0.5375 - val_accuracy: 0.5697\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5496 - accuracy: 0.5458 - val_loss: 0.5302 - val_accuracy: 0.5631\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5423 - accuracy: 0.5568 - val_loss: 0.5300 - val_accuracy: 0.5317\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5471 - accuracy: 0.5584 - val_loss: 0.5251 - val_accuracy: 0.6092\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5438 - accuracy: 0.5626 - val_loss: 0.5597 - val_accuracy: 0.5070\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5464 - accuracy: 0.5549 - val_loss: 0.5239 - val_accuracy: 0.5523\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5404 - accuracy: 0.5659 - val_loss: 0.5032 - val_accuracy: 0.6158\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5335 - accuracy: 0.5757 - val_loss: 0.5069 - val_accuracy: 0.5688\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5458 - accuracy: 0.5604 - val_loss: 0.5239 - val_accuracy: 0.6010\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5404 - accuracy: 0.5582 - val_loss: 0.5268 - val_accuracy: 0.5878\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5367 - accuracy: 0.5612 - val_loss: 0.5199 - val_accuracy: 0.5944\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5389 - accuracy: 0.5584 - val_loss: 0.5160 - val_accuracy: 0.5853\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5395 - accuracy: 0.5601 - val_loss: 0.5227 - val_accuracy: 0.5796\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5333 - accuracy: 0.5576 - val_loss: 0.5102 - val_accuracy: 0.6183\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5312 - accuracy: 0.5678 - val_loss: 0.5130 - val_accuracy: 0.5944\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5324 - accuracy: 0.5735 - val_loss: 0.5203 - val_accuracy: 0.5804\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5312 - accuracy: 0.5763 - val_loss: 0.5141 - val_accuracy: 0.6200\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5304 - accuracy: 0.5697 - val_loss: 0.5335 - val_accuracy: 0.5911\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5290 - accuracy: 0.5788 - val_loss: 0.4950 - val_accuracy: 0.6290\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5326 - accuracy: 0.5694 - val_loss: 0.5078 - val_accuracy: 0.5911\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5291 - accuracy: 0.5790 - val_loss: 0.5027 - val_accuracy: 0.6167\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5305 - accuracy: 0.5760 - val_loss: 0.5201 - val_accuracy: 0.5688\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5295 - accuracy: 0.5741 - val_loss: 0.4981 - val_accuracy: 0.6076\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5340 - accuracy: 0.5689 - val_loss: 0.5203 - val_accuracy: 0.5548\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5318 - accuracy: 0.5837 - val_loss: 0.5043 - val_accuracy: 0.5977\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5232 - accuracy: 0.5906 - val_loss: 0.4884 - val_accuracy: 0.6356\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5274 - accuracy: 0.5757 - val_loss: 0.5246 - val_accuracy: 0.5977\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5309 - accuracy: 0.5895 - val_loss: 0.4837 - val_accuracy: 0.6373\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5295 - accuracy: 0.5760 - val_loss: 0.5179 - val_accuracy: 0.5639\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5231 - accuracy: 0.5837 - val_loss: 0.5202 - val_accuracy: 0.6010\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5231 - accuracy: 0.5870 - val_loss: 0.5007 - val_accuracy: 0.6109\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5217 - accuracy: 0.5917 - val_loss: 0.4993 - val_accuracy: 0.5589\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5248 - accuracy: 0.5878 - val_loss: 0.5212 - val_accuracy: 0.6167\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5184 - accuracy: 0.5867 - val_loss: 0.4986 - val_accuracy: 0.6059\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5149 - accuracy: 0.5909 - val_loss: 0.4961 - val_accuracy: 0.6051\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5143 - accuracy: 0.5900 - val_loss: 0.4892 - val_accuracy: 0.6472\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5155 - accuracy: 0.5933 - val_loss: 0.5004 - val_accuracy: 0.6059\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5217 - accuracy: 0.5771 - val_loss: 0.5369 - val_accuracy: 0.5276\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5135 - accuracy: 0.5983 - val_loss: 0.5005 - val_accuracy: 0.6076\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5200 - accuracy: 0.5829 - val_loss: 0.4859 - val_accuracy: 0.6035\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5183 - accuracy: 0.5810 - val_loss: 0.5006 - val_accuracy: 0.6002\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5142 - accuracy: 0.5832 - val_loss: 0.4987 - val_accuracy: 0.6051\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5177 - accuracy: 0.5925 - val_loss: 0.4790 - val_accuracy: 0.6389\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5129 - accuracy: 0.6027 - val_loss: 0.4777 - val_accuracy: 0.6068\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5113 - accuracy: 0.5870 - val_loss: 0.5070 - val_accuracy: 0.5639\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5114 - accuracy: 0.5900 - val_loss: 0.4904 - val_accuracy: 0.5894\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5083 - accuracy: 0.5928 - val_loss: 0.4749 - val_accuracy: 0.6455\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5103 - accuracy: 0.5936 - val_loss: 0.4930 - val_accuracy: 0.6249\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5163 - accuracy: 0.5801 - val_loss: 0.4805 - val_accuracy: 0.5977\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5122 - accuracy: 0.5972 - val_loss: 0.4647 - val_accuracy: 0.6472\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5121 - accuracy: 0.6019 - val_loss: 0.4762 - val_accuracy: 0.6101\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5102 - accuracy: 0.5854 - val_loss: 0.4720 - val_accuracy: 0.6282\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 8s 10ms/step - loss: 0.5104 - accuracy: 0.5955 - val_loss: 0.4721 - val_accuracy: 0.6472\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5050 - accuracy: 0.5986 - val_loss: 0.4783 - val_accuracy: 0.6208\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5029 - accuracy: 0.5958 - val_loss: 0.4688 - val_accuracy: 0.6356\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5002 - accuracy: 0.6046 - val_loss: 0.4696 - val_accuracy: 0.6373\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5123 - accuracy: 0.5867 - val_loss: 0.6958 - val_accuracy: 0.3998\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5152 - accuracy: 0.5958 - val_loss: 0.4754 - val_accuracy: 0.6447\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.4993 - accuracy: 0.6060 - val_loss: 0.4741 - val_accuracy: 0.6373\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5073 - accuracy: 0.5889 - val_loss: 0.4669 - val_accuracy: 0.6043\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.4978 - accuracy: 0.6008 - val_loss: 0.4806 - val_accuracy: 0.6158\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5018 - accuracy: 0.6109 - val_loss: 0.4947 - val_accuracy: 0.5894\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5041 - accuracy: 0.6021 - val_loss: 0.4506 - val_accuracy: 0.6554\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5058 - accuracy: 0.5969 - val_loss: 0.4671 - val_accuracy: 0.6331\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5006 - accuracy: 0.6068 - val_loss: 0.5144 - val_accuracy: 0.5730\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5080 - accuracy: 0.6019 - val_loss: 0.4758 - val_accuracy: 0.6455\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5009 - accuracy: 0.6112 - val_loss: 0.4623 - val_accuracy: 0.6496\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4891 - accuracy: 0.6296 - val_loss: 0.4628 - val_accuracy: 0.6216\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4904 - accuracy: 0.6244 - val_loss: 0.4613 - val_accuracy: 0.6315\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5024 - accuracy: 0.6079 - val_loss: 0.4718 - val_accuracy: 0.6587\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5014 - accuracy: 0.6082 - val_loss: 0.4619 - val_accuracy: 0.6422\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4951 - accuracy: 0.6109 - val_loss: 0.4722 - val_accuracy: 0.6183\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4960 - accuracy: 0.6096 - val_loss: 0.4614 - val_accuracy: 0.6562\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4940 - accuracy: 0.6219 - val_loss: 0.5053 - val_accuracy: 0.6389\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4931 - accuracy: 0.6079 - val_loss: 0.4498 - val_accuracy: 0.6513\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4943 - accuracy: 0.6096 - val_loss: 0.4636 - val_accuracy: 0.6455\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5001 - accuracy: 0.5994 - val_loss: 0.4938 - val_accuracy: 0.6084\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4964 - accuracy: 0.6112 - val_loss: 0.4593 - val_accuracy: 0.6422\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4889 - accuracy: 0.6200 - val_loss: 0.4587 - val_accuracy: 0.6406\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.4936 - accuracy: 0.6162 - val_loss: 0.4489 - val_accuracy: 0.6735\n",
            "243/243 [==============================] - 1s 3ms/step - loss: 0.4489 - accuracy: 0.6735\n"
          ]
        }
      ],
      "source": [
        "# Learning rates to iterate over\n",
        "lr_to_test = np.arange(0.01, 0.1, 0.01)\n",
        "\n",
        "# Define early_stopping_monitor\n",
        "#early_stopping_monitor = EarlyStopping(patience=2)\n",
        "\n",
        "# Initialize variables recording score, accuracy, and Stop_epoch\n",
        "score = np.zeros(len(lr_to_test))\n",
        "acc = np.zeros(len(lr_to_test))\n",
        "Stop_epoch = np.zeros(len(lr_to_test))\n",
        "\n",
        "print('Training LSTM...')\n",
        "\n",
        "batch_size = 5  # Adjust batch size as needed\n",
        "\n",
        "# Loop over learning rates\n",
        "for i, lr in enumerate(lr_to_test):\n",
        "    print('\\n\\nTesting model with learning rate: %f\\n' % lr)\n",
        "\n",
        "    # Build new model to test, unaffected by previous models\n",
        "    model = get_new_model()  # Assuming you have a function that returns a new, untrained model\n",
        "\n",
        "    # Create SGD optimizer with specified learning rate\n",
        "    my_optimizer = SGD(learning_rate=lr)\n",
        "\n",
        "    # Build the optimizer with the model's trainable variables\n",
        "    my_optimizer.build(model.trainable_variables)\n",
        "\n",
        "    # Compile the model with the optimizer\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer=my_optimizer, metrics=['accuracy'])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=my_optimizer, metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    model.fit(X_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=100,\n",
        "              validation_data=(X_test, y_test))#,\n",
        "              #callbacks=[early_stopping_monitor])\n",
        "\n",
        "    # Evaluate the fitted model using the accuracy metric\n",
        "    score[i], acc[i] = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "    # Record the epoch where training was stopped for each learning rate\n",
        "    #Stop_epoch[i] = early_stopping_monitor.stopped_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VElRx9gIEpLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fcaf325-82a3-496a-e8f0-0aa5f4efe1d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_16 (LSTM)              (None, 1, 15)             4620      \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 1, 15)             60        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 1, 15)             0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1, 15)             240       \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              (None, 15)                1860      \n",
            "                                                                 \n",
            " batch_normalization_17 (Ba  (None, 15)                60        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 15)                0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 15)                240       \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 3)                 48        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7128 (27.84 KB)\n",
            "Trainable params: 7068 (27.61 KB)\n",
            "Non-trainable params: 60 (240.00 Byte)\n",
            "_________________________________________________________________\n",
            "243/243 [==============================] - 1s 4ms/step - loss: 0.4489 - accuracy: 0.6735\n",
            "score: 0.44885027408599854\n",
            "accuracy: 0.6735366582870483\n",
            "38/38 [==============================] - 1s 5ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.11      0.19       168\n",
            "           1       0.70      0.74      0.72       577\n",
            "           2       0.63      0.79      0.71       468\n",
            "\n",
            "    accuracy                           0.67      1213\n",
            "   macro avg       0.75      0.55      0.54      1213\n",
            "weighted avg       0.70      0.67      0.64      1213\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's look at a summary of the model\n",
        "model.summary()\n",
        "# Evaluate the fitted moel using the accuracy metric\n",
        "scores, accu = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "print('score:', scores)\n",
        "print('accuracy:', accu)\n",
        "\n",
        "# Modelinizin tahminlerini yapın\n",
        "y_pred_prob = model.predict(X_test)  # Sınıf olasılıklarını elde etmek için\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)  # En yüksek olasılığa sahip sınıfı seçmek için\n",
        "\n",
        "# Gerçek etiketleri uygun biçime dönüştürün\n",
        "y_test_cat = np.argmax(y_test, axis=1)  # One-hot encoded'i geri dönüştürmek için\n",
        "\n",
        "# Classification raporu oluşturun\n",
        "report = classification_report(y_test_cat, y_pred)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi-LsVMiFDgd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "43353dc7-be6d-4721-b98f-0fcd549b0ed1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, host = plt.subplots()\n",
        "par1 = host.twinx()\n",
        "\n",
        "p1, = host.plot(lr_to_test, score, \"b-\", label=\"Validation Loss\")\n",
        "p2, = par1.plot(lr_to_test, Stop_epoch, \"r-\", label=\"Stop Epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX5D4IX5Fj-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "c856c31d-78ce-462b-8fb9-60b1bb0ed2e6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, host = plt.subplots()\n",
        "par1 = host.twinx()\n",
        "\n",
        "p1, = host.plot(lr_to_test, acc, \"b-\", label=\"Accuracy\")\n",
        "p2, = par1.plot(lr_to_test, score, \"r-\", label=\"Stop Epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('score:', score)\n",
        "print('accuracy:', acc)\n",
        "\n",
        "# Modelinizin tahminlerini yapın\n",
        "y_pred_prob = model.predict(X_test)  # Sınıf olasılıklarını elde etmek için\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)  # En yüksek olasılığa sahip sınıfı seçmek için\n",
        "\n",
        "# Gerçek etiketleri uygun biçime dönüştürün\n",
        "y_test_cat = np.argmax(y_test, axis=1)  # One-hot encoded'i geri dönüştürmek için\n",
        "\n",
        "# Classification raporu oluşturun\n",
        "report = classification_report(y_test_cat, y_pred)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "miqcNer-LYML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3776588-9be9-484d-b64e-5631d9e2bb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score: [0.47050795 0.46719298 0.46413904 0.4362855  0.46374634 0.47461683\n",
            " 0.4751201  0.46546471 0.44885027]\n",
            "accuracy: [0.63314098 0.64633143 0.65952182 0.68260509 0.67600989 0.66694146\n",
            " 0.65952182 0.64056057 0.67353666]\n",
            "38/38 [==============================] - 0s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.11      0.19       168\n",
            "           1       0.70      0.74      0.72       577\n",
            "           2       0.63      0.79      0.71       468\n",
            "\n",
            "    accuracy                           0.67      1213\n",
            "   macro avg       0.75      0.55      0.54      1213\n",
            "weighted avg       0.70      0.67      0.64      1213\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tekli lstm"
      ],
      "metadata": {
        "id": "2kntQEGjiyH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training LSTM...')\n",
        "\n",
        "batch_size = 5 # I think I want to use batch_size = 10\n",
        "\n",
        "# Now fit the model to the training data\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=200,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the fitted moel using the accuracy metric\n",
        "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "print('score:', score)\n",
        "print('accuracy:', acc)\n",
        "\n",
        "# Modelinizin tahminlerini yapın\n",
        "y_pred_prob = model.predict(X_test)  # Sınıf olasılıklarını elde etmek için\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)  # En yüksek olasılığa sahip sınıfı seçmek için\n",
        "\n",
        "# Gerçek etiketleri uygun biçime dönüştürün\n",
        "y_test_cat = np.argmax(y_test, axis=1)  # One-hot encoded'i geri dönüştürmek için\n",
        "\n",
        "# Classification raporu oluşturun\n",
        "report = classification_report(y_test_cat, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Sbx9rmVJiuCP",
        "outputId": "e7a7c7d2-4d79-4c27-f03a-430f78ae0ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-edfc6af53097>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Now fit the model to the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEyG-KL7E3_-",
        "outputId": "ab339af7-c500-4863-a728-3555361c763d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.21      0.34       168\n",
            "           1       0.66      0.89      0.76       577\n",
            "           2       0.70      0.59      0.64       468\n",
            "\n",
            "    accuracy                           0.68      1213\n",
            "   macro avg       0.74      0.56      0.58      1213\n",
            "weighted avg       0.70      0.68      0.65      1213\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJbRw0yjmSn2",
        "outputId": "f508d240-3fad-4cb3-caf6-38f216e933eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00727476, 0.41087974, 0.0072783 , ..., 0.70434119, 0.05677185,\n",
              "        0.66666667],\n",
              "       [0.00728122, 0.41142446, 0.00728306, ..., 0.57174605, 0.04334705,\n",
              "        0.33333333],\n",
              "       [0.00729629, 0.41192349, 0.00728334, ..., 0.52887795, 0.0421311 ,\n",
              "        0.66666667],\n",
              "       ...,\n",
              "       [0.00778358, 0.41199271, 0.00775209, ..., 0.36863288, 0.0592997 ,\n",
              "        0.33333333],\n",
              "       [0.00787831, 0.41164204, 0.00795413, ..., 0.65314798, 0.07387497,\n",
              "        1.        ],\n",
              "       [0.00778358, 0.41199271, 0.00775209, ..., 0.36863288, 0.0592997 ,\n",
              "        0.33333333]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary"
      ],
      "metadata": {
        "id": "GFaHGwGBpqKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There will be 15 neurons in the LSTM network, one for each input.\n",
        "num_neurons = 15\n",
        "num_features = 61\n",
        "\n",
        "def get_binary_model():\n",
        "\n",
        "    # Create the model instance and define the network structure\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=True))\n",
        "    # Consider removing the Flatten() layer to maintain sequential information\n",
        "\n",
        "    # Add Batch Normalization and Dropout between layers\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(num_neurons, activation='relu'))\n",
        "\n",
        "    # Adjust the input shape for the second LSTM or other layers\n",
        "    model.add(LSTM(num_neurons, activation=\"relu\", return_sequences=False))\n",
        "    # Add Batch Normalization and Dropout between layers\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(num_neurons, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kyWP643sprrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the data into a 75% train / 25% test split.\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y_binary, test_size=0.25, random_state=36)\n",
        "# Reshape the input variables to fit the expected input shape\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
      ],
      "metadata": {
        "id": "Uu--0LyamB97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training LSTM...')\n",
        "lr=0.03\n",
        "    # Build new model to test, unaffected by previous models\n",
        "#model = get_new_model()  # Assuming you have a function that returns a new, untrained model\n",
        "model = get_binary_model()\n",
        "\n",
        "    # Create SGD optimizer with specified learning rate\n",
        "my_optimizer = SGD(learning_rate=lr)\n",
        "\n",
        "    # Build the optimizer with the model's trainable variables\n",
        "my_optimizer.build(model.trainable_variables)\n",
        "\n",
        "    # Compile the model with the optimizer\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer=my_optimizer, metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "batch_size = 5 # I think I want to use batch_size = 10\n",
        "\n",
        "# Now fit the model to the training data\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=100,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the fitted moel using the accuracy metric\n",
        "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "print('score:', score)\n",
        "print('accuracy:', acc)\n",
        "\n",
        "# Modelinizin tahminlerini yapın\n",
        "y_pred_prob = model.predict(X_test)  # Sınıf olasılıklarını elde etmek için\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)  # En yüksek olasılığa sahip sınıfı seçmek için\n",
        "\n",
        "# Gerçek etiketleri uygun biçime dönüştürün\n",
        "y_test_cat = np.argmax(y_test, axis=1)  # One-hot encoded'i geri dönüştürmek için\n",
        "\n",
        "# Classification raporu oluşturun\n",
        "report = classification_report(y_test_cat, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e167692-f7bc-429e-cf28-f2edcea5e45f",
        "id": "J2b46gwcmAPD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM...\n",
            "Epoch 1/100\n",
            "728/728 [==============================] - 12s 10ms/step - loss: 0.6948 - accuracy: 0.5466 - val_loss: 0.6804 - val_accuracy: 0.5730\n",
            "Epoch 2/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.6832 - accuracy: 0.5637 - val_loss: 0.6554 - val_accuracy: 0.6043\n",
            "Epoch 3/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.6775 - accuracy: 0.5914 - val_loss: 0.6649 - val_accuracy: 0.6167\n",
            "Epoch 4/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.6653 - accuracy: 0.6101 - val_loss: 0.6590 - val_accuracy: 0.6307\n",
            "Epoch 5/100\n",
            "728/728 [==============================] - 8s 10ms/step - loss: 0.6669 - accuracy: 0.6002 - val_loss: 0.6452 - val_accuracy: 0.6035\n",
            "Epoch 6/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.6626 - accuracy: 0.6181 - val_loss: 0.6187 - val_accuracy: 0.6546\n",
            "Epoch 7/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.6554 - accuracy: 0.6244 - val_loss: 0.6256 - val_accuracy: 0.6455\n",
            "Epoch 8/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.6551 - accuracy: 0.6206 - val_loss: 0.6286 - val_accuracy: 0.6298\n",
            "Epoch 9/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.6444 - accuracy: 0.6299 - val_loss: 0.6302 - val_accuracy: 0.6513\n",
            "Epoch 10/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.6436 - accuracy: 0.6346 - val_loss: 0.6108 - val_accuracy: 0.6562\n",
            "Epoch 11/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.6376 - accuracy: 0.6373 - val_loss: 0.5935 - val_accuracy: 0.6744\n",
            "Epoch 12/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.6301 - accuracy: 0.6401 - val_loss: 0.6006 - val_accuracy: 0.6430\n",
            "Epoch 13/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.6335 - accuracy: 0.6335 - val_loss: 0.5878 - val_accuracy: 0.6711\n",
            "Epoch 14/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.6228 - accuracy: 0.6525 - val_loss: 0.5890 - val_accuracy: 0.6826\n",
            "Epoch 15/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.6226 - accuracy: 0.6412 - val_loss: 0.5945 - val_accuracy: 0.6653\n",
            "Epoch 16/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.6281 - accuracy: 0.6467 - val_loss: 0.5792 - val_accuracy: 0.6867\n",
            "Epoch 17/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.6216 - accuracy: 0.6445 - val_loss: 0.5759 - val_accuracy: 0.6851\n",
            "Epoch 18/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.6089 - accuracy: 0.6640 - val_loss: 0.5509 - val_accuracy: 0.7040\n",
            "Epoch 19/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.6079 - accuracy: 0.6632 - val_loss: 0.5720 - val_accuracy: 0.6999\n",
            "Epoch 20/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.6134 - accuracy: 0.6497 - val_loss: 0.5544 - val_accuracy: 0.7189\n",
            "Epoch 21/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5977 - accuracy: 0.6610 - val_loss: 0.5530 - val_accuracy: 0.7082\n",
            "Epoch 22/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.6076 - accuracy: 0.6602 - val_loss: 0.5520 - val_accuracy: 0.7123\n",
            "Epoch 23/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5960 - accuracy: 0.6676 - val_loss: 0.5668 - val_accuracy: 0.6843\n",
            "Epoch 24/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.6044 - accuracy: 0.6577 - val_loss: 0.5446 - val_accuracy: 0.7246\n",
            "Epoch 25/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.6020 - accuracy: 0.6657 - val_loss: 0.5419 - val_accuracy: 0.6941\n",
            "Epoch 26/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.6020 - accuracy: 0.6615 - val_loss: 0.5462 - val_accuracy: 0.7296\n",
            "Epoch 27/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5971 - accuracy: 0.6607 - val_loss: 0.5304 - val_accuracy: 0.7288\n",
            "Epoch 28/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.5833 - accuracy: 0.6849 - val_loss: 0.5290 - val_accuracy: 0.7090\n",
            "Epoch 29/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5892 - accuracy: 0.6805 - val_loss: 0.5226 - val_accuracy: 0.7312\n",
            "Epoch 30/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5926 - accuracy: 0.6753 - val_loss: 0.5091 - val_accuracy: 0.7370\n",
            "Epoch 31/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5874 - accuracy: 0.6657 - val_loss: 0.5069 - val_accuracy: 0.7362\n",
            "Epoch 32/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5902 - accuracy: 0.6731 - val_loss: 0.5225 - val_accuracy: 0.7362\n",
            "Epoch 33/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5882 - accuracy: 0.6742 - val_loss: 0.5216 - val_accuracy: 0.7362\n",
            "Epoch 34/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5675 - accuracy: 0.6896 - val_loss: 0.5082 - val_accuracy: 0.7271\n",
            "Epoch 35/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5795 - accuracy: 0.6835 - val_loss: 0.5186 - val_accuracy: 0.7139\n",
            "Epoch 36/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5771 - accuracy: 0.6846 - val_loss: 0.5326 - val_accuracy: 0.7279\n",
            "Epoch 37/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5728 - accuracy: 0.6877 - val_loss: 0.5180 - val_accuracy: 0.7304\n",
            "Epoch 38/100\n",
            "728/728 [==============================] - 8s 10ms/step - loss: 0.5606 - accuracy: 0.6934 - val_loss: 0.5079 - val_accuracy: 0.7362\n",
            "Epoch 39/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5539 - accuracy: 0.6989 - val_loss: 0.5191 - val_accuracy: 0.7131\n",
            "Epoch 40/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5618 - accuracy: 0.6885 - val_loss: 0.5095 - val_accuracy: 0.7502\n",
            "Epoch 41/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5647 - accuracy: 0.6918 - val_loss: 0.4868 - val_accuracy: 0.7535\n",
            "Epoch 42/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5607 - accuracy: 0.6976 - val_loss: 0.4943 - val_accuracy: 0.7568\n",
            "Epoch 43/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5551 - accuracy: 0.7006 - val_loss: 0.5150 - val_accuracy: 0.7626\n",
            "Epoch 44/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5541 - accuracy: 0.6995 - val_loss: 0.5455 - val_accuracy: 0.7370\n",
            "Epoch 45/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5491 - accuracy: 0.7080 - val_loss: 0.5249 - val_accuracy: 0.7519\n",
            "Epoch 46/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5452 - accuracy: 0.7055 - val_loss: 0.5000 - val_accuracy: 0.7444\n",
            "Epoch 47/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5510 - accuracy: 0.7036 - val_loss: 0.5447 - val_accuracy: 0.7032\n",
            "Epoch 48/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5462 - accuracy: 0.7105 - val_loss: 0.4958 - val_accuracy: 0.7683\n",
            "Epoch 49/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5468 - accuracy: 0.7080 - val_loss: 0.5465 - val_accuracy: 0.7585\n",
            "Epoch 50/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5479 - accuracy: 0.7094 - val_loss: 0.5198 - val_accuracy: 0.7411\n",
            "Epoch 51/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5378 - accuracy: 0.7184 - val_loss: 0.4690 - val_accuracy: 0.7815\n",
            "Epoch 52/100\n",
            "728/728 [==============================] - 8s 11ms/step - loss: 0.5337 - accuracy: 0.7182 - val_loss: 0.4812 - val_accuracy: 0.7708\n",
            "Epoch 53/100\n",
            "728/728 [==============================] - 9s 13ms/step - loss: 0.5407 - accuracy: 0.7102 - val_loss: 0.5261 - val_accuracy: 0.7090\n",
            "Epoch 54/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5336 - accuracy: 0.7198 - val_loss: 0.4604 - val_accuracy: 0.7634\n",
            "Epoch 55/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5363 - accuracy: 0.7253 - val_loss: 0.4554 - val_accuracy: 0.7774\n",
            "Epoch 56/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.5389 - accuracy: 0.7127 - val_loss: 0.4609 - val_accuracy: 0.7906\n",
            "Epoch 57/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5213 - accuracy: 0.7248 - val_loss: 0.4792 - val_accuracy: 0.7700\n",
            "Epoch 58/100\n",
            "728/728 [==============================] - 5s 8ms/step - loss: 0.5364 - accuracy: 0.7020 - val_loss: 0.4763 - val_accuracy: 0.7650\n",
            "Epoch 59/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5232 - accuracy: 0.7220 - val_loss: 0.4644 - val_accuracy: 0.7840\n",
            "Epoch 60/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5251 - accuracy: 0.7171 - val_loss: 0.4731 - val_accuracy: 0.7502\n",
            "Epoch 61/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5167 - accuracy: 0.7245 - val_loss: 0.4573 - val_accuracy: 0.7617\n",
            "Epoch 62/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5191 - accuracy: 0.7283 - val_loss: 0.4565 - val_accuracy: 0.8071\n",
            "Epoch 63/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5281 - accuracy: 0.7234 - val_loss: 0.4621 - val_accuracy: 0.7659\n",
            "Epoch 64/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5192 - accuracy: 0.7270 - val_loss: 0.4560 - val_accuracy: 0.7799\n",
            "Epoch 65/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5223 - accuracy: 0.7223 - val_loss: 0.4350 - val_accuracy: 0.7898\n",
            "Epoch 66/100\n",
            "728/728 [==============================] - 8s 11ms/step - loss: 0.5114 - accuracy: 0.7341 - val_loss: 0.4976 - val_accuracy: 0.7683\n",
            "Epoch 67/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5152 - accuracy: 0.7245 - val_loss: 0.4595 - val_accuracy: 0.7568\n",
            "Epoch 68/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.5067 - accuracy: 0.7399 - val_loss: 0.4895 - val_accuracy: 0.7601\n",
            "Epoch 69/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5156 - accuracy: 0.7338 - val_loss: 0.4410 - val_accuracy: 0.7848\n",
            "Epoch 70/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5060 - accuracy: 0.7454 - val_loss: 0.4593 - val_accuracy: 0.7840\n",
            "Epoch 71/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4974 - accuracy: 0.7316 - val_loss: 0.4137 - val_accuracy: 0.7799\n",
            "Epoch 72/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.5064 - accuracy: 0.7338 - val_loss: 0.4549 - val_accuracy: 0.7923\n",
            "Epoch 73/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4998 - accuracy: 0.7462 - val_loss: 0.4594 - val_accuracy: 0.7865\n",
            "Epoch 74/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.5076 - accuracy: 0.7349 - val_loss: 0.4440 - val_accuracy: 0.7873\n",
            "Epoch 75/100\n",
            "728/728 [==============================] - 8s 10ms/step - loss: 0.5012 - accuracy: 0.7415 - val_loss: 0.4383 - val_accuracy: 0.7997\n",
            "Epoch 76/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4952 - accuracy: 0.7382 - val_loss: 0.4400 - val_accuracy: 0.7848\n",
            "Epoch 77/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4986 - accuracy: 0.7363 - val_loss: 0.4578 - val_accuracy: 0.7873\n",
            "Epoch 78/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.4938 - accuracy: 0.7547 - val_loss: 0.4306 - val_accuracy: 0.7791\n",
            "Epoch 79/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4906 - accuracy: 0.7564 - val_loss: 0.4300 - val_accuracy: 0.7848\n",
            "Epoch 80/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.5011 - accuracy: 0.7366 - val_loss: 0.4477 - val_accuracy: 0.8005\n",
            "Epoch 81/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4875 - accuracy: 0.7459 - val_loss: 0.4142 - val_accuracy: 0.7972\n",
            "Epoch 82/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4981 - accuracy: 0.7388 - val_loss: 0.4274 - val_accuracy: 0.7914\n",
            "Epoch 83/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4915 - accuracy: 0.7509 - val_loss: 0.4633 - val_accuracy: 0.7873\n",
            "Epoch 84/100\n",
            "728/728 [==============================] - 5s 7ms/step - loss: 0.4972 - accuracy: 0.7352 - val_loss: 0.4330 - val_accuracy: 0.8120\n",
            "Epoch 85/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.4867 - accuracy: 0.7437 - val_loss: 0.4468 - val_accuracy: 0.7972\n",
            "Epoch 86/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4947 - accuracy: 0.7429 - val_loss: 0.4577 - val_accuracy: 0.7741\n",
            "Epoch 87/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4963 - accuracy: 0.7413 - val_loss: 0.4471 - val_accuracy: 0.7997\n",
            "Epoch 88/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.4898 - accuracy: 0.7512 - val_loss: 0.4246 - val_accuracy: 0.8071\n",
            "Epoch 89/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.4830 - accuracy: 0.7520 - val_loss: 0.4311 - val_accuracy: 0.7923\n",
            "Epoch 90/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4851 - accuracy: 0.7523 - val_loss: 0.4163 - val_accuracy: 0.8071\n",
            "Epoch 91/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.4806 - accuracy: 0.7487 - val_loss: 0.4361 - val_accuracy: 0.7774\n",
            "Epoch 92/100\n",
            "728/728 [==============================] - 8s 11ms/step - loss: 0.4837 - accuracy: 0.7457 - val_loss: 0.4873 - val_accuracy: 0.7337\n",
            "Epoch 93/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4927 - accuracy: 0.7418 - val_loss: 0.4026 - val_accuracy: 0.8046\n",
            "Epoch 94/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4622 - accuracy: 0.7569 - val_loss: 0.4033 - val_accuracy: 0.8145\n",
            "Epoch 95/100\n",
            "728/728 [==============================] - 7s 10ms/step - loss: 0.4772 - accuracy: 0.7509 - val_loss: 0.4343 - val_accuracy: 0.8013\n",
            "Epoch 96/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4786 - accuracy: 0.7542 - val_loss: 0.4192 - val_accuracy: 0.7997\n",
            "Epoch 97/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4898 - accuracy: 0.7451 - val_loss: 0.4304 - val_accuracy: 0.8178\n",
            "Epoch 98/100\n",
            "728/728 [==============================] - 7s 9ms/step - loss: 0.4719 - accuracy: 0.7575 - val_loss: 0.4715 - val_accuracy: 0.7436\n",
            "Epoch 99/100\n",
            "728/728 [==============================] - 6s 8ms/step - loss: 0.4756 - accuracy: 0.7608 - val_loss: 0.4221 - val_accuracy: 0.7980\n",
            "Epoch 100/100\n",
            "728/728 [==============================] - 6s 9ms/step - loss: 0.4686 - accuracy: 0.7567 - val_loss: 0.4145 - val_accuracy: 0.8228\n",
            "243/243 [==============================] - 1s 3ms/step - loss: 0.4145 - accuracy: 0.8228\n",
            "score: 0.41446036100387573\n",
            "accuracy: 0.8227534890174866\n",
            "38/38 [==============================] - 1s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.84       636\n",
            "           1       0.84      0.78      0.81       577\n",
            "\n",
            "    accuracy                           0.82      1213\n",
            "   macro avg       0.82      0.82      0.82      1213\n",
            "weighted avg       0.82      0.82      0.82      1213\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary ama featuresız"
      ],
      "metadata": {
        "id": "pGY3b73n5HdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There will be 15 neurons in the LSTM network, one for each input.\n",
        "num_neurons = 15\n",
        "num_features = 1\n",
        "\n",
        "def get_raw_model():\n",
        "\n",
        "    # Create the model instance and define the network structure\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=True))\n",
        "    # Consider removing the Flatten() layer to maintain sequential information\n",
        "\n",
        "    # Add Batch Normalization and Dropout between layers\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(num_neurons, activation='relu'))\n",
        "\n",
        "    # Adjust the input shape for the second LSTM or other layers\n",
        "    model.add(LSTM(num_neurons, activation=\"relu\", return_sequences=False))\n",
        "    # Add Batch Normalization and Dropout between layers\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(num_neurons, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "USDSqLhFBHd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix to feed into the LSTM model\n",
        "X = eda_raw.values\n",
        "y = to_categorical(eda_raw_label)\n",
        "\n",
        "# Scale the data between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Separate the data into a 75% train / 25% test split.\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size=0.25, random_state=36)\n",
        "# Reshape the input variables to fit the expected input shape\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "lr=0.03\n",
        "    # Build new model to test, unaffected by previous models\n",
        "#model = get_new_model()  # Assuming you have a function that returns a new, untrained model\n",
        "model = get_raw_model()\n",
        "\n",
        "    # Create SGD optimizer with specified learning rate\n",
        "my_optimizer = SGD(learning_rate=lr)\n",
        "\n",
        "    # Build the optimizer with the model's trainable variables\n",
        "my_optimizer.build(model.trainable_variables)\n",
        "\n",
        "    # Compile the model with the optimizer\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer=my_optimizer, metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "batch_size = 10 # I think I want to use batch_size = 10\n",
        "\n",
        "# Now fit the model to the training data\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=100,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the fitted moel using the accuracy metric\n",
        "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "print('score:', score)\n",
        "print('accuracy:', acc)\n",
        "\n",
        "# Modelinizin tahminlerini yapın\n",
        "y_pred_prob = model.predict(X_test)  # Sınıf olasılıklarını elde etmek için\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)  # En yüksek olasılığa sahip sınıfı seçmek için\n",
        "\n",
        "# Gerçek etiketleri uygun biçime dönüştürün\n",
        "y_test_cat = np.argmax(y_test, axis=1)  # One-hot encoded'i geri dönüştürmek için\n",
        "\n",
        "# Classification raporu oluşturun\n",
        "report = classification_report(y_test_cat, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr1NreVs5Mqj",
        "outputId": "61125786-53fe-4d04-f096-45c6d4ae7adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1350/1350 [==============================] - 16s 8ms/step - loss: 0.6206 - accuracy: 0.4045 - val_loss: 0.6078 - val_accuracy: 0.4380\n",
            "Epoch 2/100\n",
            "1350/1350 [==============================] - 8s 6ms/step - loss: 0.6096 - accuracy: 0.4167 - val_loss: 0.6085 - val_accuracy: 0.4042\n",
            "Epoch 3/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6072 - accuracy: 0.4192 - val_loss: 0.6006 - val_accuracy: 0.4240\n",
            "Epoch 4/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6063 - accuracy: 0.4133 - val_loss: 0.6073 - val_accuracy: 0.4222\n",
            "Epoch 5/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6055 - accuracy: 0.4226 - val_loss: 0.6026 - val_accuracy: 0.4147\n",
            "Epoch 6/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6052 - accuracy: 0.4221 - val_loss: 0.6003 - val_accuracy: 0.4291\n",
            "Epoch 7/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6055 - accuracy: 0.4233 - val_loss: 0.6012 - val_accuracy: 0.4107\n",
            "Epoch 8/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6042 - accuracy: 0.4206 - val_loss: 0.6032 - val_accuracy: 0.4224\n",
            "Epoch 9/100\n",
            "1350/1350 [==============================] - 13s 9ms/step - loss: 0.6046 - accuracy: 0.4196 - val_loss: 0.6028 - val_accuracy: 0.4238\n",
            "Epoch 10/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6033 - accuracy: 0.4199 - val_loss: 0.6003 - val_accuracy: 0.4533\n",
            "Epoch 11/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6033 - accuracy: 0.4233 - val_loss: 0.6005 - val_accuracy: 0.4287\n",
            "Epoch 12/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6027 - accuracy: 0.4252 - val_loss: 0.6028 - val_accuracy: 0.4204\n",
            "Epoch 13/100\n",
            "1350/1350 [==============================] - 12s 9ms/step - loss: 0.6025 - accuracy: 0.4229 - val_loss: 0.5997 - val_accuracy: 0.4311\n",
            "Epoch 14/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6022 - accuracy: 0.4261 - val_loss: 0.6020 - val_accuracy: 0.4204\n",
            "Epoch 15/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6013 - accuracy: 0.4253 - val_loss: 0.6011 - val_accuracy: 0.4160\n",
            "Epoch 16/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.6020 - accuracy: 0.4227 - val_loss: 0.6030 - val_accuracy: 0.4318\n",
            "Epoch 17/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6020 - accuracy: 0.4190 - val_loss: 0.6046 - val_accuracy: 0.4191\n",
            "Epoch 18/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6009 - accuracy: 0.4230 - val_loss: 0.5984 - val_accuracy: 0.4282\n",
            "Epoch 19/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6007 - accuracy: 0.4220 - val_loss: 0.5996 - val_accuracy: 0.4242\n",
            "Epoch 20/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.6010 - accuracy: 0.4247 - val_loss: 0.6004 - val_accuracy: 0.4220\n",
            "Epoch 21/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6005 - accuracy: 0.4260 - val_loss: 0.5966 - val_accuracy: 0.4311\n",
            "Epoch 22/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6003 - accuracy: 0.4242 - val_loss: 0.6098 - val_accuracy: 0.4084\n",
            "Epoch 23/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.6005 - accuracy: 0.4261 - val_loss: 0.6027 - val_accuracy: 0.4336\n",
            "Epoch 24/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6009 - accuracy: 0.4238 - val_loss: 0.5968 - val_accuracy: 0.4229\n",
            "Epoch 25/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6009 - accuracy: 0.4279 - val_loss: 0.5964 - val_accuracy: 0.4353\n",
            "Epoch 26/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6002 - accuracy: 0.4247 - val_loss: 0.5962 - val_accuracy: 0.4373\n",
            "Epoch 27/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6007 - accuracy: 0.4238 - val_loss: 0.5980 - val_accuracy: 0.4284\n",
            "Epoch 28/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6008 - accuracy: 0.4276 - val_loss: 0.5968 - val_accuracy: 0.4340\n",
            "Epoch 29/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6009 - accuracy: 0.4256 - val_loss: 0.5998 - val_accuracy: 0.4247\n",
            "Epoch 30/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.6002 - accuracy: 0.4241 - val_loss: 0.5949 - val_accuracy: 0.4400\n",
            "Epoch 31/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6004 - accuracy: 0.4230 - val_loss: 0.5960 - val_accuracy: 0.4367\n",
            "Epoch 32/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.6002 - accuracy: 0.4259 - val_loss: 0.5964 - val_accuracy: 0.4360\n",
            "Epoch 33/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6006 - accuracy: 0.4222 - val_loss: 0.6010 - val_accuracy: 0.4204\n",
            "Epoch 34/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6002 - accuracy: 0.4257 - val_loss: 0.5963 - val_accuracy: 0.4353\n",
            "Epoch 35/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6007 - accuracy: 0.4244 - val_loss: 0.5966 - val_accuracy: 0.4333\n",
            "Epoch 36/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6001 - accuracy: 0.4253 - val_loss: 0.5950 - val_accuracy: 0.4409\n",
            "Epoch 37/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6006 - accuracy: 0.4263 - val_loss: 0.5964 - val_accuracy: 0.4236\n",
            "Epoch 38/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6008 - accuracy: 0.4211 - val_loss: 0.6004 - val_accuracy: 0.4202\n",
            "Epoch 39/100\n",
            "1350/1350 [==============================] - 9s 6ms/step - loss: 0.6012 - accuracy: 0.4269 - val_loss: 0.5953 - val_accuracy: 0.4382\n",
            "Epoch 40/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5996 - accuracy: 0.4323 - val_loss: 0.6048 - val_accuracy: 0.4204\n",
            "Epoch 41/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5995 - accuracy: 0.4292 - val_loss: 0.5949 - val_accuracy: 0.4596\n",
            "Epoch 42/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5997 - accuracy: 0.4253 - val_loss: 0.6027 - val_accuracy: 0.4431\n",
            "Epoch 43/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6003 - accuracy: 0.4259 - val_loss: 0.6006 - val_accuracy: 0.4427\n",
            "Epoch 44/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6000 - accuracy: 0.4269 - val_loss: 0.5997 - val_accuracy: 0.4298\n",
            "Epoch 45/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6009 - accuracy: 0.4256 - val_loss: 0.5962 - val_accuracy: 0.4536\n",
            "Epoch 46/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6000 - accuracy: 0.4244 - val_loss: 0.5949 - val_accuracy: 0.4280\n",
            "Epoch 47/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5998 - accuracy: 0.4294 - val_loss: 0.5943 - val_accuracy: 0.4458\n",
            "Epoch 48/100\n",
            "1350/1350 [==============================] - 12s 9ms/step - loss: 0.5998 - accuracy: 0.4296 - val_loss: 0.5949 - val_accuracy: 0.4387\n",
            "Epoch 49/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5995 - accuracy: 0.4219 - val_loss: 0.5942 - val_accuracy: 0.4444\n",
            "Epoch 50/100\n",
            "1350/1350 [==============================] - 12s 9ms/step - loss: 0.5998 - accuracy: 0.4243 - val_loss: 0.5957 - val_accuracy: 0.4013\n",
            "Epoch 51/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5991 - accuracy: 0.4257 - val_loss: 0.5942 - val_accuracy: 0.4424\n",
            "Epoch 52/100\n",
            "1350/1350 [==============================] - 12s 9ms/step - loss: 0.5992 - accuracy: 0.4251 - val_loss: 0.5941 - val_accuracy: 0.4349\n",
            "Epoch 53/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5992 - accuracy: 0.4297 - val_loss: 0.5947 - val_accuracy: 0.4396\n",
            "Epoch 54/100\n",
            "1350/1350 [==============================] - 13s 9ms/step - loss: 0.6003 - accuracy: 0.4263 - val_loss: 0.6107 - val_accuracy: 0.4084\n",
            "Epoch 55/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5995 - accuracy: 0.4267 - val_loss: 0.6011 - val_accuracy: 0.4269\n",
            "Epoch 56/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6000 - accuracy: 0.4259 - val_loss: 0.6049 - val_accuracy: 0.4424\n",
            "Epoch 57/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5993 - accuracy: 0.4293 - val_loss: 0.5952 - val_accuracy: 0.4349\n",
            "Epoch 58/100\n",
            "1350/1350 [==============================] - 12s 9ms/step - loss: 0.6000 - accuracy: 0.4223 - val_loss: 0.5990 - val_accuracy: 0.4280\n",
            "Epoch 59/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6002 - accuracy: 0.4250 - val_loss: 0.6002 - val_accuracy: 0.4282\n",
            "Epoch 60/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5993 - accuracy: 0.4292 - val_loss: 0.5967 - val_accuracy: 0.4236\n",
            "Epoch 61/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5999 - accuracy: 0.4228 - val_loss: 0.5938 - val_accuracy: 0.4451\n",
            "Epoch 62/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5992 - accuracy: 0.4289 - val_loss: 0.5945 - val_accuracy: 0.4382\n",
            "Epoch 63/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.5995 - accuracy: 0.4301 - val_loss: 0.6005 - val_accuracy: 0.4258\n",
            "Epoch 64/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5994 - accuracy: 0.4240 - val_loss: 0.6115 - val_accuracy: 0.4151\n",
            "Epoch 65/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6000 - accuracy: 0.4250 - val_loss: 0.5942 - val_accuracy: 0.4462\n",
            "Epoch 66/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5983 - accuracy: 0.4300 - val_loss: 0.5951 - val_accuracy: 0.4347\n",
            "Epoch 67/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.5996 - accuracy: 0.4292 - val_loss: 0.5994 - val_accuracy: 0.4293\n",
            "Epoch 68/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5987 - accuracy: 0.4270 - val_loss: 0.5944 - val_accuracy: 0.4344\n",
            "Epoch 69/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5989 - accuracy: 0.4279 - val_loss: 0.5934 - val_accuracy: 0.4476\n",
            "Epoch 70/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5994 - accuracy: 0.4264 - val_loss: 0.5953 - val_accuracy: 0.4369\n",
            "Epoch 71/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5989 - accuracy: 0.4298 - val_loss: 0.5967 - val_accuracy: 0.4373\n",
            "Epoch 72/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.6000 - accuracy: 0.4256 - val_loss: 0.5938 - val_accuracy: 0.4473\n",
            "Epoch 73/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5994 - accuracy: 0.4247 - val_loss: 0.5954 - val_accuracy: 0.4364\n",
            "Epoch 74/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5994 - accuracy: 0.4279 - val_loss: 0.5976 - val_accuracy: 0.4329\n",
            "Epoch 75/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.6005 - accuracy: 0.4258 - val_loss: 0.6051 - val_accuracy: 0.4267\n",
            "Epoch 76/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5996 - accuracy: 0.4216 - val_loss: 0.5985 - val_accuracy: 0.4318\n",
            "Epoch 77/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5991 - accuracy: 0.4250 - val_loss: 0.5968 - val_accuracy: 0.4322\n",
            "Epoch 78/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.6003 - accuracy: 0.4263 - val_loss: 0.5935 - val_accuracy: 0.4380\n",
            "Epoch 79/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5996 - accuracy: 0.4241 - val_loss: 0.5939 - val_accuracy: 0.4369\n",
            "Epoch 80/100\n",
            "1350/1350 [==============================] - 12s 9ms/step - loss: 0.5994 - accuracy: 0.4293 - val_loss: 0.5997 - val_accuracy: 0.4309\n",
            "Epoch 81/100\n",
            "1350/1350 [==============================] - 12s 9ms/step - loss: 0.5983 - accuracy: 0.4333 - val_loss: 0.5962 - val_accuracy: 0.4342\n",
            "Epoch 82/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5987 - accuracy: 0.4288 - val_loss: 0.5939 - val_accuracy: 0.4442\n",
            "Epoch 83/100\n",
            "1350/1350 [==============================] - 12s 9ms/step - loss: 0.5986 - accuracy: 0.4292 - val_loss: 0.5988 - val_accuracy: 0.4344\n",
            "Epoch 84/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5985 - accuracy: 0.4284 - val_loss: 0.5941 - val_accuracy: 0.4349\n",
            "Epoch 85/100\n",
            "1350/1350 [==============================] - 12s 9ms/step - loss: 0.5987 - accuracy: 0.4303 - val_loss: 0.5963 - val_accuracy: 0.4382\n",
            "Epoch 86/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5997 - accuracy: 0.4288 - val_loss: 0.5968 - val_accuracy: 0.4347\n",
            "Epoch 87/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5989 - accuracy: 0.4266 - val_loss: 0.5935 - val_accuracy: 0.4347\n",
            "Epoch 88/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5992 - accuracy: 0.4259 - val_loss: 0.5938 - val_accuracy: 0.4382\n",
            "Epoch 89/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5985 - accuracy: 0.4272 - val_loss: 0.5953 - val_accuracy: 0.4589\n",
            "Epoch 90/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.5996 - accuracy: 0.4270 - val_loss: 0.6011 - val_accuracy: 0.4284\n",
            "Epoch 91/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5983 - accuracy: 0.4289 - val_loss: 0.5989 - val_accuracy: 0.4320\n",
            "Epoch 92/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.5996 - accuracy: 0.4322 - val_loss: 0.5992 - val_accuracy: 0.4309\n",
            "Epoch 93/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.5984 - accuracy: 0.4291 - val_loss: 0.5964 - val_accuracy: 0.4342\n",
            "Epoch 94/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5994 - accuracy: 0.4271 - val_loss: 0.5960 - val_accuracy: 0.4311\n",
            "Epoch 95/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.5981 - accuracy: 0.4224 - val_loss: 0.5953 - val_accuracy: 0.4331\n",
            "Epoch 96/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5990 - accuracy: 0.4281 - val_loss: 0.5980 - val_accuracy: 0.4318\n",
            "Epoch 97/100\n",
            "1350/1350 [==============================] - 10s 7ms/step - loss: 0.5995 - accuracy: 0.4288 - val_loss: 0.5951 - val_accuracy: 0.4320\n",
            "Epoch 98/100\n",
            "1350/1350 [==============================] - 10s 8ms/step - loss: 0.5980 - accuracy: 0.4302 - val_loss: 0.5968 - val_accuracy: 0.4307\n",
            "Epoch 99/100\n",
            "1350/1350 [==============================] - 9s 7ms/step - loss: 0.5986 - accuracy: 0.4264 - val_loss: 0.5965 - val_accuracy: 0.4336\n",
            "Epoch 100/100\n",
            "1350/1350 [==============================] - 11s 8ms/step - loss: 0.5990 - accuracy: 0.4316 - val_loss: 0.5991 - val_accuracy: 0.4311\n",
            "450/450 [==============================] - 1s 3ms/step - loss: 0.5991 - accuracy: 0.4311\n",
            "score: 0.5990871787071228\n",
            "accuracy: 0.43111109733581543\n",
            "141/141 [==============================] - 1s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       922\n",
            "           1       0.41      0.98      0.58      1786\n",
            "           2       0.84      0.11      0.19      1792\n",
            "\n",
            "    accuracy                           0.43      4500\n",
            "   macro avg       0.42      0.36      0.26      4500\n",
            "weighted avg       0.50      0.43      0.30      4500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(eda_raw_label)\n",
        "eda_raw_label_binary = [0 if x == 2 else x for x in eda_raw_label]"
      ],
      "metadata": {
        "id": "xJ1IBR2fj1Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(eda_raw_label_binary)\n",
        "benzersiz_sayilar = list(set(eda_raw_label_binary))\n",
        "for sayi in benzersiz_sayilar:\n",
        "    sayi_count = eda_raw_label_binary.count(sayi)\n",
        "    print(f\"{sayi} sayısı listede {sayi_count} kez geçiyor.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSyS5agL7b69",
        "outputId": "da4dc722-8063-4d58-f254-4c18a10b23ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 sayısı listede 10800 kez geçiyor.\n",
            "1.0 sayısı listede 7200 kez geçiyor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eda_raw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "afJEe10ojO2R",
        "outputId": "74587850-912f-47e8-9386-9c55134ce858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e329d8c5b944>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meda_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'eda_raw' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix to feed into the LSTM model\n",
        "X = eda_raw.values\n",
        "y = to_categorical(eda_raw_label_binary)\n",
        "\n",
        "# Scale the data between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Separate the data into a 75% train / 25% test split.\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size=0.25, random_state=36)\n",
        "# Reshape the input variables to fit the expected input shape\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "lr=0.03\n",
        "    # Build new model to test, unaffected by previous models\n",
        "#model = get_new_model()  # Assuming you have a function that returns a new, untrained model\n",
        "model = get_binary_model()\n",
        "\n",
        "    # Create SGD optimizer with specified learning rate\n",
        "my_optimizer = SGD(learning_rate=lr)\n",
        "\n",
        "    # Build the optimizer with the model's trainable variables\n",
        "my_optimizer.build(model.trainable_variables)\n",
        "\n",
        "    # Compile the model with the optimizer\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer=my_optimizer, metrics=['accuracy'])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "batch_size = 32 # I think I want to use batch_size = 10\n",
        "\n",
        "# Now fit the model to the training data\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=200,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the fitted moel using the accuracy metric\n",
        "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "print('score:', score)\n",
        "print('accuracy:', acc)\n",
        "\n",
        "# Modelinizin tahminlerini yapın\n",
        "y_pred_prob = model.predict(X_test)  # Sınıf olasılıklarını elde etmek için\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)  # En yüksek olasılığa sahip sınıfı seçmek için\n",
        "\n",
        "# Gerçek etiketleri uygun biçime dönüştürün\n",
        "y_test_cat = np.argmax(y_test, axis=1)  # One-hot encoded'i geri dönüştürmek için\n",
        "\n",
        "# Classification raporu oluşturun\n",
        "report = classification_report(y_test_cat, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lICKPB-jsD4",
        "outputId": "95b696dc-c83b-4d7b-d876-541d4311b868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "422/422 [==============================] - 10s 10ms/step - loss: 0.6780 - accuracy: 0.5912 - val_loss: 0.6720 - val_accuracy: 0.6031\n",
            "Epoch 2/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6720 - accuracy: 0.5990 - val_loss: 0.6694 - val_accuracy: 0.6031\n",
            "Epoch 3/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6711 - accuracy: 0.5990 - val_loss: 0.6692 - val_accuracy: 0.6031\n",
            "Epoch 4/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6698 - accuracy: 0.5990 - val_loss: 0.6629 - val_accuracy: 0.6031\n",
            "Epoch 5/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6697 - accuracy: 0.5990 - val_loss: 0.6639 - val_accuracy: 0.6031\n",
            "Epoch 6/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6685 - accuracy: 0.5990 - val_loss: 0.6634 - val_accuracy: 0.6031\n",
            "Epoch 7/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6684 - accuracy: 0.5990 - val_loss: 0.6613 - val_accuracy: 0.6031\n",
            "Epoch 8/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6674 - accuracy: 0.5990 - val_loss: 0.6628 - val_accuracy: 0.6031\n",
            "Epoch 9/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6673 - accuracy: 0.5990 - val_loss: 0.6608 - val_accuracy: 0.6031\n",
            "Epoch 10/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6666 - accuracy: 0.5990 - val_loss: 0.6626 - val_accuracy: 0.6031\n",
            "Epoch 11/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6676 - accuracy: 0.5990 - val_loss: 0.6643 - val_accuracy: 0.6031\n",
            "Epoch 12/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6656 - accuracy: 0.5990 - val_loss: 0.6676 - val_accuracy: 0.6031\n",
            "Epoch 13/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6656 - accuracy: 0.5990 - val_loss: 0.6602 - val_accuracy: 0.6031\n",
            "Epoch 14/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6647 - accuracy: 0.5990 - val_loss: 0.6591 - val_accuracy: 0.6031\n",
            "Epoch 15/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6646 - accuracy: 0.5990 - val_loss: 0.6590 - val_accuracy: 0.6031\n",
            "Epoch 16/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6652 - accuracy: 0.5990 - val_loss: 0.6580 - val_accuracy: 0.6031\n",
            "Epoch 17/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6627 - accuracy: 0.5990 - val_loss: 0.6658 - val_accuracy: 0.6031\n",
            "Epoch 18/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6635 - accuracy: 0.5990 - val_loss: 0.6580 - val_accuracy: 0.6031\n",
            "Epoch 19/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6654 - accuracy: 0.5990 - val_loss: 0.6579 - val_accuracy: 0.6031\n",
            "Epoch 20/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6636 - accuracy: 0.5990 - val_loss: 0.6576 - val_accuracy: 0.6031\n",
            "Epoch 21/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6634 - accuracy: 0.5990 - val_loss: 0.6606 - val_accuracy: 0.6031\n",
            "Epoch 22/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6624 - accuracy: 0.5990 - val_loss: 0.6607 - val_accuracy: 0.6031\n",
            "Epoch 23/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6638 - accuracy: 0.5990 - val_loss: 0.6581 - val_accuracy: 0.6031\n",
            "Epoch 24/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6628 - accuracy: 0.5990 - val_loss: 0.6588 - val_accuracy: 0.6031\n",
            "Epoch 25/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6632 - accuracy: 0.5990 - val_loss: 0.6573 - val_accuracy: 0.6031\n",
            "Epoch 26/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6623 - accuracy: 0.5990 - val_loss: 0.6610 - val_accuracy: 0.6031\n",
            "Epoch 27/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6633 - accuracy: 0.5990 - val_loss: 0.6648 - val_accuracy: 0.6031\n",
            "Epoch 28/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6626 - accuracy: 0.5990 - val_loss: 0.6569 - val_accuracy: 0.6031\n",
            "Epoch 29/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6629 - accuracy: 0.5990 - val_loss: 0.6574 - val_accuracy: 0.6031\n",
            "Epoch 30/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6625 - accuracy: 0.5990 - val_loss: 0.6579 - val_accuracy: 0.6031\n",
            "Epoch 31/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6628 - accuracy: 0.5990 - val_loss: 0.6570 - val_accuracy: 0.6031\n",
            "Epoch 32/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6624 - accuracy: 0.5990 - val_loss: 0.6607 - val_accuracy: 0.6031\n",
            "Epoch 33/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6623 - accuracy: 0.5990 - val_loss: 0.6680 - val_accuracy: 0.6031\n",
            "Epoch 34/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6615 - accuracy: 0.5990 - val_loss: 0.6671 - val_accuracy: 0.6031\n",
            "Epoch 35/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6619 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 36/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6625 - accuracy: 0.5990 - val_loss: 0.6575 - val_accuracy: 0.6031\n",
            "Epoch 37/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6623 - accuracy: 0.5990 - val_loss: 0.6662 - val_accuracy: 0.6031\n",
            "Epoch 38/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6627 - accuracy: 0.5990 - val_loss: 0.6629 - val_accuracy: 0.6031\n",
            "Epoch 39/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6615 - accuracy: 0.5990 - val_loss: 0.6599 - val_accuracy: 0.6031\n",
            "Epoch 40/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6627 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 41/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6616 - accuracy: 0.5990 - val_loss: 0.6667 - val_accuracy: 0.6031\n",
            "Epoch 42/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6616 - accuracy: 0.5990 - val_loss: 0.6606 - val_accuracy: 0.6031\n",
            "Epoch 43/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6617 - accuracy: 0.5990 - val_loss: 0.6593 - val_accuracy: 0.6031\n",
            "Epoch 44/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6630 - accuracy: 0.5990 - val_loss: 0.6593 - val_accuracy: 0.6031\n",
            "Epoch 45/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6635 - accuracy: 0.5990 - val_loss: 0.6600 - val_accuracy: 0.6031\n",
            "Epoch 46/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6616 - accuracy: 0.5990 - val_loss: 0.6657 - val_accuracy: 0.6031\n",
            "Epoch 47/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6619 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 48/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6621 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 49/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6623 - accuracy: 0.5990 - val_loss: 0.6634 - val_accuracy: 0.6031\n",
            "Epoch 50/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6623 - accuracy: 0.5990 - val_loss: 0.6591 - val_accuracy: 0.6031\n",
            "Epoch 51/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6622 - accuracy: 0.5990 - val_loss: 0.6589 - val_accuracy: 0.6031\n",
            "Epoch 52/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6622 - accuracy: 0.5990 - val_loss: 0.6624 - val_accuracy: 0.6031\n",
            "Epoch 53/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6612 - accuracy: 0.5990 - val_loss: 0.6584 - val_accuracy: 0.6031\n",
            "Epoch 54/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6609 - accuracy: 0.5990 - val_loss: 0.6569 - val_accuracy: 0.6031\n",
            "Epoch 55/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6619 - accuracy: 0.5990 - val_loss: 0.6592 - val_accuracy: 0.6031\n",
            "Epoch 56/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6613 - accuracy: 0.5990 - val_loss: 0.6656 - val_accuracy: 0.6031\n",
            "Epoch 57/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6624 - accuracy: 0.5990 - val_loss: 0.6606 - val_accuracy: 0.6031\n",
            "Epoch 58/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6609 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 59/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6625 - accuracy: 0.5990 - val_loss: 0.6594 - val_accuracy: 0.6031\n",
            "Epoch 60/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6607 - accuracy: 0.5990 - val_loss: 0.6563 - val_accuracy: 0.6031\n",
            "Epoch 61/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6627 - accuracy: 0.5990 - val_loss: 0.6604 - val_accuracy: 0.6031\n",
            "Epoch 62/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6618 - accuracy: 0.5990 - val_loss: 0.6570 - val_accuracy: 0.6031\n",
            "Epoch 63/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6608 - accuracy: 0.5990 - val_loss: 0.6595 - val_accuracy: 0.6031\n",
            "Epoch 64/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6614 - accuracy: 0.5990 - val_loss: 0.6625 - val_accuracy: 0.6031\n",
            "Epoch 65/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6614 - accuracy: 0.5990 - val_loss: 0.6605 - val_accuracy: 0.6031\n",
            "Epoch 66/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6629 - accuracy: 0.5990 - val_loss: 0.6604 - val_accuracy: 0.6031\n",
            "Epoch 67/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6618 - accuracy: 0.5990 - val_loss: 0.6572 - val_accuracy: 0.6031\n",
            "Epoch 68/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6612 - accuracy: 0.5990 - val_loss: 0.6581 - val_accuracy: 0.6031\n",
            "Epoch 69/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6616 - accuracy: 0.5990 - val_loss: 0.6652 - val_accuracy: 0.6031\n",
            "Epoch 70/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6620 - accuracy: 0.5990 - val_loss: 0.6598 - val_accuracy: 0.6031\n",
            "Epoch 71/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6608 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 72/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6612 - accuracy: 0.5990 - val_loss: 0.6619 - val_accuracy: 0.6031\n",
            "Epoch 73/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6620 - accuracy: 0.5990 - val_loss: 0.6617 - val_accuracy: 0.6031\n",
            "Epoch 74/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6607 - accuracy: 0.5990 - val_loss: 0.6624 - val_accuracy: 0.6031\n",
            "Epoch 75/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6612 - accuracy: 0.5990 - val_loss: 0.6577 - val_accuracy: 0.6031\n",
            "Epoch 76/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6625 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 77/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6619 - accuracy: 0.5990 - val_loss: 0.6562 - val_accuracy: 0.6031\n",
            "Epoch 78/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6619 - accuracy: 0.5990 - val_loss: 0.6593 - val_accuracy: 0.6031\n",
            "Epoch 79/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6618 - accuracy: 0.5990 - val_loss: 0.6576 - val_accuracy: 0.6031\n",
            "Epoch 80/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6622 - accuracy: 0.5990 - val_loss: 0.6634 - val_accuracy: 0.6031\n",
            "Epoch 81/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6613 - accuracy: 0.5990 - val_loss: 0.6577 - val_accuracy: 0.6031\n",
            "Epoch 82/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6605 - accuracy: 0.5990 - val_loss: 0.6583 - val_accuracy: 0.6031\n",
            "Epoch 83/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6615 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 84/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6615 - accuracy: 0.5990 - val_loss: 0.6603 - val_accuracy: 0.6031\n",
            "Epoch 85/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6610 - accuracy: 0.5990 - val_loss: 0.6627 - val_accuracy: 0.6031\n",
            "Epoch 86/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6623 - accuracy: 0.5990 - val_loss: 0.6570 - val_accuracy: 0.6031\n",
            "Epoch 87/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6623 - accuracy: 0.5990 - val_loss: 0.6564 - val_accuracy: 0.6031\n",
            "Epoch 88/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6603 - accuracy: 0.5990 - val_loss: 0.6585 - val_accuracy: 0.6031\n",
            "Epoch 89/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6613 - accuracy: 0.5990 - val_loss: 0.6570 - val_accuracy: 0.6031\n",
            "Epoch 90/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6611 - accuracy: 0.5990 - val_loss: 0.6608 - val_accuracy: 0.6031\n",
            "Epoch 91/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6615 - accuracy: 0.5990 - val_loss: 0.6585 - val_accuracy: 0.6031\n",
            "Epoch 92/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6614 - accuracy: 0.5990 - val_loss: 0.6611 - val_accuracy: 0.6031\n",
            "Epoch 93/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6615 - accuracy: 0.5990 - val_loss: 0.6558 - val_accuracy: 0.6031\n",
            "Epoch 94/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6612 - accuracy: 0.5990 - val_loss: 0.6557 - val_accuracy: 0.6031\n",
            "Epoch 95/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6619 - accuracy: 0.5990 - val_loss: 0.6617 - val_accuracy: 0.6031\n",
            "Epoch 96/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6616 - accuracy: 0.5990 - val_loss: 0.6612 - val_accuracy: 0.6031\n",
            "Epoch 97/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6606 - accuracy: 0.5990 - val_loss: 0.6586 - val_accuracy: 0.6031\n",
            "Epoch 98/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6613 - accuracy: 0.5990 - val_loss: 0.6661 - val_accuracy: 0.6031\n",
            "Epoch 99/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6602 - accuracy: 0.5990 - val_loss: 0.6560 - val_accuracy: 0.6031\n",
            "Epoch 100/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6605 - accuracy: 0.5990 - val_loss: 0.6592 - val_accuracy: 0.6031\n",
            "Epoch 101/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6614 - accuracy: 0.5990 - val_loss: 0.6584 - val_accuracy: 0.6031\n",
            "Epoch 102/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6611 - accuracy: 0.5990 - val_loss: 0.6622 - val_accuracy: 0.6031\n",
            "Epoch 103/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6610 - accuracy: 0.5990 - val_loss: 0.6560 - val_accuracy: 0.6031\n",
            "Epoch 104/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6618 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 105/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6614 - accuracy: 0.5990 - val_loss: 0.6579 - val_accuracy: 0.6031\n",
            "Epoch 106/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6607 - accuracy: 0.5990 - val_loss: 0.6643 - val_accuracy: 0.6031\n",
            "Epoch 107/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6609 - accuracy: 0.5990 - val_loss: 0.6580 - val_accuracy: 0.6031\n",
            "Epoch 108/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6607 - accuracy: 0.5990 - val_loss: 0.6590 - val_accuracy: 0.6031\n",
            "Epoch 109/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6610 - accuracy: 0.5990 - val_loss: 0.6587 - val_accuracy: 0.6031\n",
            "Epoch 110/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6615 - accuracy: 0.5990 - val_loss: 0.6601 - val_accuracy: 0.6031\n",
            "Epoch 111/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6606 - accuracy: 0.5990 - val_loss: 0.6567 - val_accuracy: 0.6031\n",
            "Epoch 112/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6607 - accuracy: 0.5990 - val_loss: 0.6617 - val_accuracy: 0.6031\n",
            "Epoch 113/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6607 - accuracy: 0.5990 - val_loss: 0.6557 - val_accuracy: 0.6031\n",
            "Epoch 114/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6605 - accuracy: 0.5990 - val_loss: 0.6576 - val_accuracy: 0.6031\n",
            "Epoch 115/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6604 - accuracy: 0.5990 - val_loss: 0.6583 - val_accuracy: 0.6031\n",
            "Epoch 116/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6613 - accuracy: 0.5990 - val_loss: 0.6566 - val_accuracy: 0.6031\n",
            "Epoch 117/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6609 - accuracy: 0.5990 - val_loss: 0.6574 - val_accuracy: 0.6031\n",
            "Epoch 118/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6614 - accuracy: 0.5990 - val_loss: 0.6558 - val_accuracy: 0.6031\n",
            "Epoch 119/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6602 - accuracy: 0.5990 - val_loss: 0.6574 - val_accuracy: 0.6031\n",
            "Epoch 120/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6603 - accuracy: 0.5996 - val_loss: 0.6551 - val_accuracy: 0.6104\n",
            "Epoch 121/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6607 - accuracy: 0.5993 - val_loss: 0.6632 - val_accuracy: 0.6031\n",
            "Epoch 122/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6611 - accuracy: 0.6001 - val_loss: 0.6622 - val_accuracy: 0.6142\n",
            "Epoch 123/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6605 - accuracy: 0.6007 - val_loss: 0.6570 - val_accuracy: 0.6113\n",
            "Epoch 124/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6610 - accuracy: 0.6022 - val_loss: 0.6613 - val_accuracy: 0.5987\n",
            "Epoch 125/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6619 - accuracy: 0.5986 - val_loss: 0.6588 - val_accuracy: 0.6007\n",
            "Epoch 126/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6615 - accuracy: 0.6007 - val_loss: 0.6675 - val_accuracy: 0.5927\n",
            "Epoch 127/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6602 - accuracy: 0.6005 - val_loss: 0.6553 - val_accuracy: 0.6073\n",
            "Epoch 128/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6603 - accuracy: 0.6021 - val_loss: 0.6543 - val_accuracy: 0.6124\n",
            "Epoch 129/200\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.6605 - accuracy: 0.5993 - val_loss: 0.6548 - val_accuracy: 0.6096\n",
            "Epoch 130/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6601 - accuracy: 0.6010 - val_loss: 0.6543 - val_accuracy: 0.6153\n",
            "Epoch 131/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6600 - accuracy: 0.6021 - val_loss: 0.6567 - val_accuracy: 0.6131\n",
            "Epoch 132/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6618 - accuracy: 0.6030 - val_loss: 0.6582 - val_accuracy: 0.6178\n",
            "Epoch 133/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6612 - accuracy: 0.6034 - val_loss: 0.6673 - val_accuracy: 0.5880\n",
            "Epoch 134/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6612 - accuracy: 0.6010 - val_loss: 0.6572 - val_accuracy: 0.6040\n",
            "Epoch 135/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6606 - accuracy: 0.6010 - val_loss: 0.6633 - val_accuracy: 0.6013\n",
            "Epoch 136/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6603 - accuracy: 0.6028 - val_loss: 0.6550 - val_accuracy: 0.6196\n",
            "Epoch 137/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6598 - accuracy: 0.6023 - val_loss: 0.6541 - val_accuracy: 0.6136\n",
            "Epoch 138/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6614 - accuracy: 0.6003 - val_loss: 0.6599 - val_accuracy: 0.6162\n",
            "Epoch 139/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6616 - accuracy: 0.6024 - val_loss: 0.6559 - val_accuracy: 0.6153\n",
            "Epoch 140/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6601 - accuracy: 0.6028 - val_loss: 0.6537 - val_accuracy: 0.6144\n",
            "Epoch 141/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6601 - accuracy: 0.6037 - val_loss: 0.6612 - val_accuracy: 0.6042\n",
            "Epoch 142/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6601 - accuracy: 0.6020 - val_loss: 0.6568 - val_accuracy: 0.6118\n",
            "Epoch 143/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6603 - accuracy: 0.6036 - val_loss: 0.6638 - val_accuracy: 0.5940\n",
            "Epoch 144/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6602 - accuracy: 0.6033 - val_loss: 0.6580 - val_accuracy: 0.6124\n",
            "Epoch 145/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6591 - accuracy: 0.6050 - val_loss: 0.6552 - val_accuracy: 0.6160\n",
            "Epoch 146/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6601 - accuracy: 0.6028 - val_loss: 0.6593 - val_accuracy: 0.6100\n",
            "Epoch 147/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6598 - accuracy: 0.6026 - val_loss: 0.6589 - val_accuracy: 0.6122\n",
            "Epoch 148/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6608 - accuracy: 0.6042 - val_loss: 0.6565 - val_accuracy: 0.6209\n",
            "Epoch 149/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6599 - accuracy: 0.6039 - val_loss: 0.6579 - val_accuracy: 0.6164\n",
            "Epoch 150/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6607 - accuracy: 0.6025 - val_loss: 0.6577 - val_accuracy: 0.6118\n",
            "Epoch 151/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6601 - accuracy: 0.6033 - val_loss: 0.6542 - val_accuracy: 0.6131\n",
            "Epoch 152/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6599 - accuracy: 0.6061 - val_loss: 0.6629 - val_accuracy: 0.6096\n",
            "Epoch 153/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6604 - accuracy: 0.6031 - val_loss: 0.6638 - val_accuracy: 0.5989\n",
            "Epoch 154/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6604 - accuracy: 0.6030 - val_loss: 0.6566 - val_accuracy: 0.6184\n",
            "Epoch 155/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6599 - accuracy: 0.6062 - val_loss: 0.6564 - val_accuracy: 0.5971\n",
            "Epoch 156/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6614 - accuracy: 0.6011 - val_loss: 0.6612 - val_accuracy: 0.6140\n",
            "Epoch 157/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6607 - accuracy: 0.6033 - val_loss: 0.6535 - val_accuracy: 0.6187\n",
            "Epoch 158/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6606 - accuracy: 0.6033 - val_loss: 0.6556 - val_accuracy: 0.6113\n",
            "Epoch 159/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6597 - accuracy: 0.6048 - val_loss: 0.6542 - val_accuracy: 0.6169\n",
            "Epoch 160/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6588 - accuracy: 0.6042 - val_loss: 0.6526 - val_accuracy: 0.6153\n",
            "Epoch 161/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6619 - accuracy: 0.6014 - val_loss: 0.6560 - val_accuracy: 0.6147\n",
            "Epoch 162/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6595 - accuracy: 0.6052 - val_loss: 0.6616 - val_accuracy: 0.6084\n",
            "Epoch 163/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6601 - accuracy: 0.6058 - val_loss: 0.6535 - val_accuracy: 0.6149\n",
            "Epoch 164/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6597 - accuracy: 0.6029 - val_loss: 0.6586 - val_accuracy: 0.6131\n",
            "Epoch 165/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6591 - accuracy: 0.6055 - val_loss: 0.6576 - val_accuracy: 0.6082\n",
            "Epoch 166/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6599 - accuracy: 0.6041 - val_loss: 0.6620 - val_accuracy: 0.6091\n",
            "Epoch 167/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6595 - accuracy: 0.6047 - val_loss: 0.6621 - val_accuracy: 0.6042\n",
            "Epoch 168/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6622 - accuracy: 0.6013 - val_loss: 0.6545 - val_accuracy: 0.6187\n",
            "Epoch 169/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6599 - accuracy: 0.6054 - val_loss: 0.6575 - val_accuracy: 0.6167\n",
            "Epoch 170/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6598 - accuracy: 0.6049 - val_loss: 0.6547 - val_accuracy: 0.6176\n",
            "Epoch 171/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6599 - accuracy: 0.6033 - val_loss: 0.6535 - val_accuracy: 0.6173\n",
            "Epoch 172/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6602 - accuracy: 0.6022 - val_loss: 0.6536 - val_accuracy: 0.6187\n",
            "Epoch 173/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6606 - accuracy: 0.6019 - val_loss: 0.6644 - val_accuracy: 0.5936\n",
            "Epoch 174/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6600 - accuracy: 0.6034 - val_loss: 0.6565 - val_accuracy: 0.6127\n",
            "Epoch 175/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6596 - accuracy: 0.6056 - val_loss: 0.6582 - val_accuracy: 0.6153\n",
            "Epoch 176/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6610 - accuracy: 0.6031 - val_loss: 0.6558 - val_accuracy: 0.6173\n",
            "Epoch 177/200\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.6607 - accuracy: 0.6024 - val_loss: 0.6547 - val_accuracy: 0.6124\n",
            "Epoch 178/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6590 - accuracy: 0.6038 - val_loss: 0.6562 - val_accuracy: 0.6136\n",
            "Epoch 179/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6598 - accuracy: 0.6044 - val_loss: 0.6539 - val_accuracy: 0.6198\n",
            "Epoch 180/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6595 - accuracy: 0.6039 - val_loss: 0.6763 - val_accuracy: 0.5711\n",
            "Epoch 181/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6595 - accuracy: 0.6026 - val_loss: 0.6571 - val_accuracy: 0.5964\n",
            "Epoch 182/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6591 - accuracy: 0.6047 - val_loss: 0.6532 - val_accuracy: 0.6264\n",
            "Epoch 183/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6590 - accuracy: 0.6031 - val_loss: 0.6539 - val_accuracy: 0.6236\n",
            "Epoch 184/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6604 - accuracy: 0.6043 - val_loss: 0.6541 - val_accuracy: 0.6160\n",
            "Epoch 185/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6588 - accuracy: 0.6055 - val_loss: 0.6634 - val_accuracy: 0.6047\n",
            "Epoch 186/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6602 - accuracy: 0.6041 - val_loss: 0.6567 - val_accuracy: 0.6091\n",
            "Epoch 187/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6597 - accuracy: 0.6018 - val_loss: 0.6598 - val_accuracy: 0.6051\n",
            "Epoch 188/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6598 - accuracy: 0.6023 - val_loss: 0.6534 - val_accuracy: 0.6251\n",
            "Epoch 189/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6582 - accuracy: 0.6050 - val_loss: 0.6657 - val_accuracy: 0.5938\n",
            "Epoch 190/200\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.6587 - accuracy: 0.6047 - val_loss: 0.6534 - val_accuracy: 0.6118\n",
            "Epoch 191/200\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.6602 - accuracy: 0.6035 - val_loss: 0.6554 - val_accuracy: 0.6269\n",
            "Epoch 192/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6594 - accuracy: 0.6043 - val_loss: 0.6570 - val_accuracy: 0.6202\n",
            "Epoch 193/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6609 - accuracy: 0.6020 - val_loss: 0.6609 - val_accuracy: 0.6082\n",
            "Epoch 194/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6591 - accuracy: 0.6039 - val_loss: 0.6558 - val_accuracy: 0.6189\n",
            "Epoch 195/200\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6594 - accuracy: 0.6041 - val_loss: 0.6570 - val_accuracy: 0.6100\n",
            "Epoch 196/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6601 - accuracy: 0.6027 - val_loss: 0.6532 - val_accuracy: 0.6209\n",
            "Epoch 197/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6605 - accuracy: 0.6028 - val_loss: 0.6575 - val_accuracy: 0.6193\n",
            "Epoch 198/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6590 - accuracy: 0.6059 - val_loss: 0.6559 - val_accuracy: 0.6253\n",
            "Epoch 199/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6591 - accuracy: 0.6045 - val_loss: 0.6557 - val_accuracy: 0.6231\n",
            "Epoch 200/200\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6603 - accuracy: 0.6044 - val_loss: 0.6537 - val_accuracy: 0.6211\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6211\n",
            "score: 0.6537018418312073\n",
            "accuracy: 0.6211110949516296\n",
            "141/141 [==============================] - 1s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.98      0.76      2714\n",
            "           1       0.74      0.07      0.13      1786\n",
            "\n",
            "    accuracy                           0.62      4500\n",
            "   macro avg       0.68      0.53      0.44      4500\n",
            "weighted avg       0.67      0.62      0.51      4500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "1EP4RLFQi_5C",
        "outputId": "4313e914-a59c-42ae-89e2-a303bf74fce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b5fec669aca1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "53f52e7f84b5eb1491c5432e20e5fe03e12eded6fd7078ab5d081f72334a5853"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}